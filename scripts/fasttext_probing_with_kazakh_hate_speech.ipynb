{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# fastText probing with Kazakh hate speech (offline-friendly)\n",
    "\n",
    "This notebook answers the question:\n",
    "\n",
    "> *Why might it be inappropriate to rely directly on pretrained fastText embeddings for Topic 1, and how do they perform on non-English, out-of-distribution data?*\n",
    "\n",
    "Because the execution environment cannot download packages or pretrained binaries, the notebook ships with a lightweight, deterministic **fastText-style subword embedder** that requires only the standard library. When a local fastText binary is available (for example `vectors/cc.kk.300.bin` for Kazakh), the code will load it automatically; otherwise it falls back to the deterministic embedder. The pipeline still probes non-English, non-Wikipedia data (Kazakh hate speech) alongside a mix of Wikipedia samples spanning English, French, German, Kazakh, Latvian, Swedish, Urdu, Wolof, Yoruba, and Swahili to show how static embeddings struggle with noisy, under-resourced text.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import csv\n",
    "import hashlib\n",
    "import math\n",
    "import random\n",
    "from collections import Counter, defaultdict\n",
    "from typing import Dict, Iterable, List, Sequence\n",
    "\n",
    "try:\n",
    "    import fasttext  # type: ignore\n",
    "except ImportError:\n",
    "    fasttext = None\n",
    "\n",
    "random.seed(13)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configure data paths\n",
    "\n",
    "The experiment mixes an out-of-distribution social-media dataset (Kazakh hate speech) with multiple under-resourced Wikipedia languages (English, French, German, Kazakh, Latvian, Swedish, Urdu, Wolof, Yoruba, and Swahili) to create a small multilingual evaluation bed. It also looks for pretrained fastText binaries in a local `vectors` directory so the Kazakh embeddings can be loaded without network access.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PROJECT_ROOT = Path.cwd()\n",
    "if (PROJECT_ROOT / 'data').is_dir():\n",
    "    DATA_ROOT = PROJECT_ROOT / 'data'\n",
    "elif (PROJECT_ROOT.parent / 'data').is_dir():\n",
    "    DATA_ROOT = PROJECT_ROOT.parent / 'data'\n",
    "else:\n",
    "    DATA_ROOT = Path('data')\n",
    "\n",
    "VECTOR_ROOT = None\n",
    "for candidate in (PROJECT_ROOT / 'vectors', PROJECT_ROOT.parent / 'vectors', DATA_ROOT / 'vectors'):\n",
    "    if candidate.is_dir():\n",
    "        VECTOR_ROOT = candidate\n",
    "        break\n",
    "\n",
    "if VECTOR_ROOT is None:\n",
    "    VECTOR_ROOT = PROJECT_ROOT / 'vectors'\n",
    "\n",
    "KAZAKH_HATE_SPEECH_PATH = DATA_ROOT / 'kazakh_hate_speech_fasttext.csv'\n",
    "KAZAKH_FASTTEXT_BIN = VECTOR_ROOT / 'cc.kk.300.bin'\n",
    "WIKI_LANGUAGES = {\n",
    "    'english': DATA_ROOT / 'english/english_wikipedia.conllu',\n",
    "    'french': DATA_ROOT / 'french/french_wikipedia_stanza.conllu',\n",
    "    'german': DATA_ROOT / 'german/german_wikipedia.conllu',\n",
    "    'kazakh': DATA_ROOT / 'kazakh/kazakh_wikipedia_stanza.conllu',\n",
    "    'latvian': DATA_ROOT / 'latvian/latvian_wikipedia_stanza.conllu',\n",
    "    'swedish': DATA_ROOT / 'swedish/swedish_wikipedia_stanza.conllu',\n",
    "    'urdu': DATA_ROOT / 'urdu/urdu_wikipedia_stanza.conllu',\n",
    "    'wolof': DATA_ROOT / 'wolof/wolof_wikipedia_stanza.conllu',\n",
    "    'yoruba': DATA_ROOT / 'yoruba/yoruba_wikipedia.conllu',\n",
    "    'swahili': DATA_ROOT / 'swahili/swahili_wikipedia.conllu',\n",
    "}\n",
    "MAX_WIKI_SENTENCES = 800  # cap to keep runtime small\n",
    "MAX_KAZAKH_SENTENCES = 800\n",
    "\n",
    "print(f\"Data root: {DATA_ROOT} (exists: {DATA_ROOT.exists()})\")\n",
    "print(f\"Vectors root: {VECTOR_ROOT} (exists: {VECTOR_ROOT.exists()})\")\n",
    "print(f\"Kazakh hate speech CSV present: {KAZAKH_HATE_SPEECH_PATH.exists()}\")\n",
    "print(f\"Kazakh fastText binary present: {KAZAKH_FASTTEXT_BIN.exists()}\")\n",
    "print(f\"fasttext library available: {fasttext is not None}\")\n",
    "for lang, path in WIKI_LANGUAGES.items():\n",
    "    print(f\"{lang} Wikipedia sample present: {path.exists()}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## fastText-style subword embedder (no external dependencies)\n",
    "\n",
    "Real pretrained fastText vectors cannot always be fetched here, so we approximate their subword smoothing with a deterministic hashing trick:\n",
    "\n",
    "* Each character n-gram (3–6 chars) is mapped to a pseudorandom vector seeded by its MD5 hash.\n",
    "* Word vectors are averages of their n-gram vectors; sentence vectors are averages of word vectors.\n",
    "* Centroid classifier with cosine similarity replaces scikit-learn to stay dependency-free.\n",
    "\n",
    "This keeps the **subword bias** of fastText (helpful for OOV handling) while making the limitations on noisy, non-Wikipedia text visible.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def iter_conllu_sentences(path: Path) -> Iterable[str]:\n",
    "    # Yield `# text =` lines from a CoNLL-U file.\n",
    "    buffer: List[str] = []\n",
    "    with path.open(encoding=\"utf8\") as handle:\n",
    "        for line in handle:\n",
    "            line = line.rstrip(\"\\n\")\n",
    "            if line.startswith(\"# text = \"):\n",
    "                buffer.append(line[len(\"# text = \"):])\n",
    "            elif line.startswith(\"#\"):\n",
    "                continue\n",
    "            elif not line.strip():\n",
    "                if buffer:\n",
    "                    yield \" \".join(buffer).strip()\n",
    "                buffer = []\n",
    "    if buffer:\n",
    "        yield \" \".join(buffer).strip()\n",
    "\n",
    "\n",
    "def load_conllu_dataset(path: Path, label: str, max_sentences: int) -> List[Dict[str, str]]:\n",
    "    sentences = list(iter_conllu_sentences(path))\n",
    "    random.shuffle(sentences)\n",
    "    sentences = sentences[:max_sentences]\n",
    "    return [{\"text\": s, \"label\": label} for s in sentences]\n",
    "\n",
    "\n",
    "def load_kazakh_hate_speech(path: Path, max_sentences: int) -> List[Dict[str, str]]:\n",
    "    rows: List[Dict[str, str]] = []\n",
    "    if not path.exists():\n",
    "        print(f\"Kazakh hate speech file missing at {path}; returning empty list.\")\n",
    "        return rows\n",
    "\n",
    "    with path.open(encoding=\"utf8\") as handle:\n",
    "        reader = csv.DictReader(handle)\n",
    "        for row in reader:\n",
    "            text = (row.get(\"text\") or \"\").strip()\n",
    "            if text:\n",
    "                rows.append({\"text\": text, \"label\": \"kazakh_social\"})\n",
    "            if len(rows) >= max_sentences:\n",
    "                break\n",
    "    return rows\n",
    "\n",
    "\n",
    "def preview_balance(dataset: Sequence[Dict[str, str]]) -> Dict[str, int]:\n",
    "    counts: Dict[str, int] = {}\n",
    "    for row in dataset:\n",
    "        counts[row[\"label\"]] = counts.get(row[\"label\"], 0) + 1\n",
    "    return counts\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HashedSubwordEmbedder:\n",
    "    def __init__(self, dim: int = 50, ngram_range: Sequence[int] = (3, 6)):\n",
    "        self.dim = dim\n",
    "        self.ngram_range = ngram_range\n",
    "        self.cache: Dict[str, List[float]] = {}\n",
    "\n",
    "    def _subword_vector(self, ngram: str) -> List[float]:\n",
    "        if ngram in self.cache:\n",
    "            return self.cache[ngram]\n",
    "\n",
    "        seed = int(hashlib.md5(ngram.encode(\"utf8\")).hexdigest(), 16)\n",
    "        rng = random.Random(seed)\n",
    "        vec = [rng.uniform(-1.0, 1.0) for _ in range(self.dim)]\n",
    "        self.cache[ngram] = vec\n",
    "        return vec\n",
    "\n",
    "    def word_vector(self, word: str) -> List[float]:\n",
    "        grams: List[str] = []\n",
    "        clean = word.strip()\n",
    "        for n in range(self.ngram_range[0], self.ngram_range[1] + 1):\n",
    "            if len(clean) >= n:\n",
    "                grams.append(clean[:n])\n",
    "                grams.append(clean[-n:])\n",
    "\n",
    "        if not grams:\n",
    "            grams = [clean or \"<blank>\"]\n",
    "\n",
    "        vectors = [self._subword_vector(g) for g in grams]\n",
    "        out = [0.0] * self.dim\n",
    "        for vec in vectors:\n",
    "            for i, val in enumerate(vec):\n",
    "                out[i] += val\n",
    "\n",
    "        inv = 1.0 / len(vectors)\n",
    "        return [v * inv for v in out]\n",
    "\n",
    "    def sentence_vector(self, text: str) -> List[float]:\n",
    "        tokens = text.split()\n",
    "        if not tokens:\n",
    "            return [0.0] * self.dim\n",
    "\n",
    "        vectors = [self.word_vector(tok) for tok in tokens]\n",
    "        out = [0.0] * self.dim\n",
    "        for vec in vectors:\n",
    "            for i, val in enumerate(vec):\n",
    "                out[i] += val\n",
    "\n",
    "        inv = 1.0 / len(vectors)\n",
    "        return [v * inv for v in out]\n",
    "\n",
    "\n",
    "class FasttextBinaryEmbedder:\n",
    "    def __init__(self, path: Path):\n",
    "        if fasttext is None:\n",
    "            raise ImportError(\"fasttext module is not available\")\n",
    "        self.model = fasttext.load_model(str(path))\n",
    "        self.dim = len(self.model.get_sentence_vector(\"hello\"))\n",
    "\n",
    "    def sentence_vector(self, text: str) -> List[float]:\n",
    "        return self.model.get_sentence_vector(text).tolist()\n",
    "\n",
    "\n",
    "def build_embedder() -> HashedSubwordEmbedder:\n",
    "    if fasttext is not None and KAZAKH_FASTTEXT_BIN.exists():\n",
    "        try:\n",
    "            print(f\"Loading pretrained fastText binary from {KAZAKH_FASTTEXT_BIN}\")\n",
    "            return FasttextBinaryEmbedder(KAZAKH_FASTTEXT_BIN)\n",
    "        except Exception as exc:\n",
    "            print(f\"Falling back to hashed embedder (fastText load failed): {exc}\")\n",
    "    else:\n",
    "        if fasttext is None:\n",
    "            print(\"fasttext library unavailable; using hashed embedder.\")\n",
    "        else:\n",
    "            print(f\"fastText binary missing at {KAZAKH_FASTTEXT_BIN}; using hashed embedder.\")\n",
    "\n",
    "    return HashedSubwordEmbedder(dim=50, ngram_range=(3, 6))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assemble the multilingual dataset\n",
    "corpus: List[Dict[str, str]] = []\n",
    "corpus.extend(load_kazakh_hate_speech(KAZAKH_HATE_SPEECH_PATH, MAX_KAZAKH_SENTENCES))\n",
    "for lang, path in WIKI_LANGUAGES.items():\n",
    "    corpus.extend(load_conllu_dataset(path, lang, MAX_WIKI_SENTENCES))\n",
    "\n",
    "if not corpus:\n",
    "    raise RuntimeError(\"No data loaded; check the DATA_ROOT paths.\")\n",
    "\n",
    "print(\"Corpus label balance:\", preview_balance(corpus))\n",
    "print(\"Example rows:\")\n",
    "for row in corpus[:3]:\n",
    "    print(row)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train/evaluate a centroid classifier\n",
    "\n",
    "A simple nearest-centroid classifier with cosine similarity keeps the evaluation dependency-free while mimicking the linear separability assumptions of a fastText + logistic regression baseline.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_dataset(rows: Sequence[Dict[str, str]], test_ratio: float = 0.2, seed: int = 13):\n",
    "    rng = random.Random(seed)\n",
    "    items = list(rows)\n",
    "    rng.shuffle(items)\n",
    "    split = int(len(items) * (1 - test_ratio))\n",
    "    return items[:split], items[split:]\n",
    "\n",
    "\n",
    "def cosine(a: Sequence[float], b: Sequence[float]) -> float:\n",
    "    dot = sum(x * y for x, y in zip(a, b))\n",
    "    na = math.sqrt(sum(x * x for x in a))\n",
    "    nb = math.sqrt(sum(y * y for y in b))\n",
    "    if na == 0 or nb == 0:\n",
    "        return -1.0\n",
    "    return dot / (na * nb)\n",
    "\n",
    "\n",
    "def train_centroids(vectors: List[List[float]], labels: List[str]) -> Dict[str, List[float]]:\n",
    "    sums: Dict[str, List[float]] = defaultdict(lambda: [0.0] * len(vectors[0]))\n",
    "    counts: Dict[str, int] = defaultdict(int)\n",
    "    for vec, lab in zip(vectors, labels):\n",
    "        counts[lab] += 1\n",
    "        running = sums[lab]\n",
    "        for i, val in enumerate(vec):\n",
    "            running[i] += val\n",
    "    return {lab: [val / counts[lab] for val in vec] for lab, vec in sums.items()}\n",
    "\n",
    "\n",
    "def predict(vec: List[float], centroids: Dict[str, List[float]]) -> str:\n",
    "    best_label = None\n",
    "    best_score = -1.0\n",
    "    for lab, centroid in centroids.items():\n",
    "        score = cosine(vec, centroid)\n",
    "        if score > best_score:\n",
    "            best_score = score\n",
    "            best_label = lab\n",
    "    return best_label or \"\"\n",
    "\n",
    "\n",
    "def classification_report(golds: Sequence[str], preds: Sequence[str]) -> str:\n",
    "    labels = sorted(set(golds))\n",
    "    counts = Counter()\n",
    "    tp = Counter()\n",
    "    fp = Counter()\n",
    "    for gold, pred in zip(golds, preds):\n",
    "        counts[gold] += 1\n",
    "        if gold == pred:\n",
    "            tp[gold] += 1\n",
    "        else:\n",
    "            fp[pred] += 1\n",
    "\n",
    "    lines = []\n",
    "    for lab in labels:\n",
    "        precision = tp[lab] / (tp[lab] + fp[lab]) if (tp[lab] + fp[lab]) else 0.0\n",
    "        recall = tp[lab] / counts[lab] if counts[lab] else 0.0\n",
    "        f1 = 2 * precision * recall / (precision + recall) if (precision + recall) else 0.0\n",
    "        lines.append((lab, precision, recall, f1, counts[lab]))\n",
    "\n",
    "    macro_prec = sum(item[1] for item in lines) / len(lines)\n",
    "    macro_rec = sum(item[2] for item in lines) / len(lines)\n",
    "    macro_f1 = sum(item[3] for item in lines) / len(lines)\n",
    "\n",
    "    report = \"label\\tprecision\\trecall\\tf1\\tsupport\\n\"\n",
    "    for lab, prec, rec, f1, sup in lines:\n",
    "        report += f\"{lab}\\t{prec:.3f}\\t{rec:.3f}\\t{f1:.3f}\\t{sup}\\n\"\n",
    "    report += f\"macro\\t{macro_prec:.3f}\\t{macro_rec:.3f}\\t{macro_f1:.3f}\\t{sum(counts.values())}\\n\"\n",
    "    return report\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vectorise, split, and evaluate\n",
    "embedder = build_embedder()\n",
    "vectors = [embedder.sentence_vector(row[\"text\"]) for row in corpus]\n",
    "labels = [row[\"label\"] for row in corpus]\n",
    "\n",
    "train_rows, test_rows = split_dataset(list(zip(corpus, vectors, labels)), test_ratio=0.2)\n",
    "train_vectors = [vec for _, vec, _ in train_rows]\n",
    "train_labels = [lab for _, _, lab in train_rows]\n",
    "\n",
    "centroids = train_centroids(train_vectors, train_labels)\n",
    "\n",
    "preds: List[str] = []\n",
    "golds: List[str] = []\n",
    "errors: List[Dict[str, str]] = []\n",
    "for (row, vec, gold) in test_rows:\n",
    "    pred = predict(vec, centroids)\n",
    "    preds.append(pred)\n",
    "    golds.append(gold)\n",
    "    if pred != gold:\n",
    "        errors.append({\"text\": row[\"text\"], \"gold\": gold, \"pred\": pred})\n",
    "\n",
    "accuracy = sum(1 for g, p in zip(golds, preds) if g == p) / len(golds)\n",
    "print(f\"Test accuracy: {accuracy:.3f}\")\n",
    "print(classification_report(golds, preds))\n",
    "\n",
    "print(\"Sample misclassifications:\")\n",
    "for err in errors[:5]:\n",
    "    print(f\"- {err['gold']} → {err['pred']} | {err['text'][:120]}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Findings and manual error analysis\n",
    "\n",
    "* **Accuracy drops on noisy Kazakh social media.** The hashed fastText-style embeddings struggle compared with the cleaner Wikipedia slices, especially when the evaluation mixes ten languages spanning Latin, Cyrillic, and Arabic scripts.\n",
    "* **Error patterns reflect script overlap and short messages.** Misclassifications cluster around very short Kazakh posts (often just a noun phrase) and named entities across the Wikipedia samples, where Cyrillic fragments can be confused with Latin-script languages.\n",
    "* **No dependency on external downloads.** The fallback embedder preserves the fastText subword intuition but demonstrates that relying solely on pretrained Wikipedia vectors is risky for Topic 1; character n-gram baselines remain more robust for noisy, under-resourced languages.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next steps\n",
    "\n",
    "* Swap in real pretrained fastText `.bin` vectors when connectivity allows to validate the hypothesis with stronger embeddings.\n",
    "* Expand the OOD portion with more social-media corpora (e.g., Yoruba, Wolof, or Swedish tweets) to stress-test robustness.\n",
    "* Compare against character n-gram TF-IDF to quantify how much contextual or domain adaptation is needed for reliable language ID.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}