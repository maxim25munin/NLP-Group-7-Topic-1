{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# fastText OOD Language Identification Experiment\n\nThis notebook follows the Question 1 guidance by evaluating pretrained fastText embeddings on five languages: Kazakh, Latvian, Swedish, Yoruba, and Urdu. Wikipedia-derived CoNLL-U data are used for Latvian, Swedish, Yoruba, and Urdu, while the OOD hate-speech/social-media corpora come from `data/kazakh_hate_speech_fasttext.csv` and `data/latvian_comments_fasttext.csv`. The goal is to illustrate how relying on pretrained fastText embeddings for language identification can break when confronting non-Wikipedia, out-of-distribution content across multiple languages."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup\n\nThe notebook expects:\n\n- Wikipedia-derived CoNLL-U files for Latvian, Swedish, Yoruba, and Urdu under `data/<lang>/*.conllu`.\n- OOD hate-speech/social-media CSV files at `data/kazakh_hate_speech_fasttext.csv` and `data/latvian_comments_fasttext.csv` with columns `text` and `label`.\n- Pretrained fastText binary models saved as `cc.<lang>.300.bin` in `models/fasttext/` (or adjust the paths below). Models are provided for all five target languages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "import json\n",
    "import random\n",
    "import warnings\n",
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "from typing import Dict, Iterable, List, Optional, Sequence, Tuple\n",
    "\n",
    "import fasttext\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reproducibility settings\nRANDOM_SEED = 13\nnp.random.seed(RANDOM_SEED)\nrandom.seed(RANDOM_SEED)\n\n# Resolve project paths regardless of where the notebook is executed\nif \"__file__\" in globals():\n    _current_dir = Path(__file__).resolve().parent\nelse:\n    _current_dir = Path.cwd().resolve()\n\n_possible_roots = [_current_dir, _current_dir.parent, _current_dir.parent.parent]\nPROJECT_ROOT = next((p for p in _possible_roots if (p / \"data\").exists()), None)\nif PROJECT_ROOT is None:\n    raise FileNotFoundError(\n        \"Could not locate the 'data' directory. Please run the notebook from the repository or ensure data is available.\"\n    )\n\nDATA_DIR = PROJECT_ROOT / \"data\"\nFASTTEXT_MODEL_DIR = PROJECT_ROOT / \"models\" / \"fasttext\"\n\n# Languages included in the Wikipedia dataset\nWIKI_LANGUAGES = [\"kazakh\", \"latvian\", \"swedish\", \"yoruba\", \"urdu\"]\nOOD_LANGUAGES = [\"kazakh\", \"latvian\"]\nLANGUAGES = sorted(set(WIKI_LANGUAGES + OOD_LANGUAGES))\n\nFASTTEXT_LANGUAGE_CODES: Dict[str, str] = {\n    \"kazakh\": \"kk\",\n    \"latvian\": \"lv\",\n    \"swedish\": \"sv\",\n    \"yoruba\": \"yo\",\n    \"urdu\": \"ur\",\n}\n\nOOD_FILES: Dict[str, Path] = {\n    \"kazakh\": DATA_DIR / \"kazakh_hate_speech_fasttext.csv\",\n    \"latvian\": DATA_DIR / \"latvian_comments_fasttext.csv\",\n}\n\n# Optional: cap the number of sentences per language to keep the notebook fast\nMAX_SENTENCES_PER_LANGUAGE: Optional[int] = 2000\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data loading helpers\n\nWe reuse the Milestone 2 preprocessing assumptions: Wikipedia sentences are stored in CoNLL-U format with a `# text = ...` field. The hate-speech corpora are simple CSVs. Language labels are derived from the parent directory names for the Wikipedia data and set explicitly for each OOD set to test language identification robustness across Kazakh and Latvian."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\nclass SentenceExample:\n    text: str\n    label: str\n\n\ndef iter_conllu_sentences(path: Path) -> Iterable[str]:\n    \"\"\"Yield raw sentence strings from a CoNLL-U file.\"\"\"\n\n    buffer: List[str] = []\n    for line in path.read_text(encoding=\"utf8\").splitlines():\n        if line.startswith(\"# text = \"):\n            buffer.append(line[len(\"# text = \" ) :])\n        elif line.startswith(\"#\"):\n            continue\n        elif not line.strip():\n            if buffer:\n                yield \" \".join(buffer).strip()\n                buffer = []\n        else:\n            continue\n    if buffer:\n        yield \" \".join(buffer).strip()\n\n\ndef load_multilingual_wikipedia(\n    data_root: Path,\n    languages: Sequence[str],\n    max_sentences_per_language: Optional[int] = None,\n    seed: int = RANDOM_SEED,\n) -> pd.DataFrame:\n    \"\"\"Load Wikipedia sentences and language labels into a DataFrame.\"\"\"\n\n    rng = random.Random(seed)\n    examples: List[SentenceExample] = []\n\n    for lang in sorted(languages):\n        lang_dir = data_root / lang\n        conllu_files = sorted(lang_dir.glob(\"*.conllu\"))\n        if not conllu_files:\n            warnings.warn(f\"No CoNLL-U files found for language: {lang}\")\n            continue\n        sentences: List[str] = []\n        for conllu in conllu_files:\n            sentences.extend(iter_conllu_sentences(conllu))\n        if max_sentences_per_language is not None:\n            rng.shuffle(sentences)\n            sentences = sentences[:max_sentences_per_language]\n        examples.extend(SentenceExample(text=s, label=lang) for s in sentences)\n\n    rng.shuffle(examples)\n    if not examples:\n        raise ValueError(\n            \"No Wikipedia sentences were loaded. Ensure data/<lang>/*.conllu files exist for the selected languages.\"\n        )\n    return pd.DataFrame([e.__dict__ for e in examples])\n\n\ndef load_hate_speech_dataset(\n    path: Path, language: str, text_column: str = \"text\", label_column: str = \"label\"\n) -> pd.DataFrame:\n    \"\"\"Load an OOD hate-speech/social-media dataset and tag it with a language label.\"\"\"\n\n    if not path.exists():\n        raise FileNotFoundError(f\"Expected OOD file for {language}: {path}\")\n\n    df = pd.read_csv(path)\n    if text_column not in df.columns:\n        raise ValueError(f\"Expected a '{text_column}' column in {path}\")\n\n    domain_col = f\"{language}_domain_label\"\n    if label_column in df.columns and label_column != domain_col:\n        df = df.rename(columns={label_column: domain_col})\n    elif label_column not in df.columns:\n        df[domain_col] = np.nan\n\n    if text_column != \"text\":\n        df = df.rename(columns={text_column: \"text\"})\n\n    df[\"label\"] = language\n    return df[[\"text\", \"label\", domain_col]]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. fastText utilities\n",
    "\n",
    "The helpers below load language-specific fastText models, convert sentences to averaged word vectors, and compute out-of-vocabulary (OOV) rates for qualitative error analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_fasttext_models(\n",
    "    model_dir: Path, languages: Sequence[str], code_lookup: Optional[Dict[str, str]] = None\n",
    ") -> Dict[str, fasttext.FastText._FastText]:\n",
    "    \"\"\"Load fastText models for the specified languages.\n",
    "\n",
    "    The function expects files named `cc.<lang>.300.bin` inside `model_dir`. If a\n",
    "    model is missing, a warning is emitted and the language is skipped.\n",
    "    \"\"\"\n",
    "\n",
    "    code_lookup = code_lookup or {}\n",
    "    models: Dict[str, fasttext.FastText._FastText] = {}\n",
    "    for lang in languages:\n",
    "        code = code_lookup.get(lang, lang[:2])\n",
    "        path = model_dir / f\"cc.{code}.300.bin\"\n",
    "        if not path.exists():\n",
    "            warnings.warn(f\"Missing fastText model: {path}\")\n",
    "            continue\n",
    "        models[lang] = fasttext.load_model(path.as_posix())\n",
    "    if not models:\n",
    "        raise FileNotFoundError(\"No fastText models were loaded. Please download cc.<lang>.300.bin files.\")\n",
    "    return models\n",
    "\n",
    "\n",
    "def get_sentence_embedding(text: str, model: fasttext.FastText._FastText) -> np.ndarray:\n",
    "    \"\"\"Compute a sentence embedding by averaging token vectors.\"\"\"\n",
    "    tokens = text.split()\n",
    "    if not tokens:\n",
    "        return np.zeros(model.get_dimension(), dtype=np.float32)\n",
    "    vectors: List[np.ndarray] = [model.get_word_vector(tok) for tok in tokens]\n",
    "    return np.mean(vectors, axis=0)\n",
    "\n",
    "\n",
    "def extract_fasttext_features(\n",
    "    texts: Sequence[str],\n",
    "    models: Dict[str, fasttext.FastText._FastText],\n",
    "    language_labels: Optional[Sequence[str]] = None,\n",
    "    language_hint: Optional[str] = None,\n",
    ") -> np.ndarray:\n",
    "    \"\"\"Convert sentences to feature matrices using language-specific models.\n",
    "\n",
    "    If `language_hint` is provided, that model is used for all texts (useful for OOD\n",
    "    Kazakh-only evaluation). Otherwise the function attempts to match each sample's\n",
    "    language label to a loaded model and will raise an error if the model is\n",
    "    missing to avoid silently falling back to an unintended language.\n",
    "    \"\"\"\n",
    "    if language_hint:\n",
    "        if language_hint not in models:\n",
    "            raise ValueError(\n",
    "                f\"language_hint={language_hint!r} not found in loaded models: {sorted(models)}\"\n",
    "            )\n",
    "        default_model = models[language_hint]\n",
    "    else:\n",
    "        default_model = None\n",
    "\n",
    "    features: List[np.ndarray] = []\n",
    "    for i, text in enumerate(texts):\n",
    "        model = None\n",
    "        if language_labels is not None and i < len(language_labels):\n",
    "            lang = language_labels[i]\n",
    "            if lang not in models:\n",
    "                raise ValueError(\n",
    "                    f\"No fastText model loaded for language {lang!r}. Provide a language_hint or load the missing model.\"\n",
    "                )\n",
    "            model = models[lang]\n",
    "        elif default_model is not None:\n",
    "            model = default_model\n",
    "        else:\n",
    "            raise ValueError(\n",
    "                \"No language labels were provided and no language_hint was set; cannot select a fastText model for embedding.\"\n",
    "            )\n",
    "        features.append(get_sentence_embedding(text, model))\n",
    "    return np.vstack(features)\n",
    "\n",
    "\n",
    "def is_in_vocabulary(word: str, model: fasttext.FastText._FastText) -> bool:\n",
    "    return model.get_word_id(word) != -1\n",
    "\n",
    "\n",
    "def calculate_oov_rate(texts: Sequence[str], model: fasttext.FastText._FastText) -> float:\n",
    "    \"\"\"Compute the average proportion of OOV tokens per sentence.\"\"\"\n",
    "    rates: List[float] = []\n",
    "    for text in texts:\n",
    "        tokens = text.split()\n",
    "        if not tokens:\n",
    "            rates.append(0.0)\n",
    "            continue\n",
    "        oov = sum(1 for tok in tokens if not is_in_vocabulary(tok, model))\n",
    "        rates.append(oov / len(tokens))\n",
    "    return float(np.mean(rates))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Model training and evaluation helpers\n",
    "\n",
    "We train a multinomial logistic regression classifier on averaged fastText embeddings and report accuracy, per-class precision/recall/F1, and confusion matrices. Additional utilities collect misclassified samples for manual inspection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_fasttext_classifier(\n",
    "    train_texts: Sequence[str],\n",
    "    train_labels: Sequence[str],\n",
    "    models: Dict[str, fasttext.FastText._FastText],\n",
    "):\n",
    "    features = extract_fasttext_features(train_texts, models, language_labels=train_labels)\n",
    "    clf = LogisticRegression(max_iter=1000, multi_class=\"multinomial\", solver=\"lbfgs\")\n",
    "    clf.fit(features, train_labels)\n",
    "    return clf, features\n",
    "\n",
    "\n",
    "def evaluate_fasttext_classifier(\n",
    "    clf: LogisticRegression,\n",
    "    texts: Sequence[str],\n",
    "    labels: Sequence[str],\n",
    "    models: Dict[str, fasttext.FastText._FastText],\n",
    "    language_hint: Optional[str] = None,\n",
    "):\n",
    "    language_labels = None if language_hint else labels\n",
    "    features = extract_fasttext_features(texts, models, language_labels=language_labels, language_hint=language_hint)\n",
    "    preds = clf.predict(features)\n",
    "    acc = accuracy_score(labels, preds)\n",
    "    report = classification_report(labels, preds, output_dict=True, zero_division=0)\n",
    "    cm = confusion_matrix(labels, preds, labels=sorted(set(labels) | set(preds)))\n",
    "    return {\"accuracy\": acc, \"report\": report, \"confusion_matrix\": cm, \"predictions\": preds}\n",
    "\n",
    "\n",
    "def collect_misclassifications(\n",
    "    texts: Sequence[str],\n",
    "    labels: Sequence[str],\n",
    "    preds: Sequence[str],\n",
    "    limit: int = 20,\n",
    ") -> pd.DataFrame:\n",
    "    indices = [i for i, (y, p) in enumerate(zip(labels, preds)) if y != p]\n",
    "    sampled = indices[:limit]\n",
    "    return pd.DataFrame(\n",
    "        {\n",
    "            \"text\": [texts[i] for i in sampled],\n",
    "            \"true_label\": [labels[i] for i in sampled],\n",
    "            \"predicted_label\": [preds[i] for i in sampled],\n",
    "            \"token_count\": [len(texts[i].split()) for i in sampled],\n",
    "        }\n",
    "    )\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Load datasets\n\nThe next cell loads the Wikipedia in-distribution (ID) data for Kazakh, Latvian, Swedish, Yoruba, and Urdu, then performs a reproducible train/validation split. It also loads the Kazakh and Latvian hate-speech/social-media OOD datasets, which are held out entirely for OOD evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ood_sets: Dict[str, pd.DataFrame] = {}\nfor lang in OOD_LANGUAGES:\n    path = OOD_FILES.get(lang)\n    if path is None:\n        warnings.warn(f\"No OOD path configured for language: {lang}\")\n        continue\n    try:\n        ood_sets[lang] = load_hate_speech_dataset(path, language=lang)\n        print(f\"Loaded {len(ood_sets[lang])} {lang} OOD sentences (held out for OOD evaluation)\")\n    except FileNotFoundError as exc:\n        warnings.warn(str(exc))\n\nif not ood_sets:\n    raise FileNotFoundError(\"No OOD datasets were loaded. Add CSVs to data/<language>_*.csv or adjust OOD_FILES.\")\n\n# Combined view for downstream evaluation\nood_df = pd.concat(ood_sets.values(), ignore_index=True)\nprint(f\"Combined OOD examples: {len(ood_df)} across {len(ood_sets)} languages\")\n\nwiki_df = load_multilingual_wikipedia(\n    DATA_DIR,\n    languages=WIKI_LANGUAGES,\n    max_sentences_per_language=MAX_SENTENCES_PER_LANGUAGE,\n    seed=RANDOM_SEED,\n)\nprint(f\"Loaded {len(wiki_df)} Wikipedia sentences across {wiki_df.label.nunique()} languages\")\n\ntrain_df, test_df = train_test_split(\n    wiki_df, test_size=0.2, random_state=RANDOM_SEED, stratify=wiki_df.label\n)\nprint(f\"Train size: {len(train_df)}, Test size: {len(test_df)}\")\n\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Load pretrained fastText models\n",
    "\n",
    "Download the `cc.<lang>.300.bin` files from the [fastText](https://fasttext.cc/docs/en/crawl-vectors.html) repository and place them in `models/fasttext/` before running this cell. All five languages used in the experiment need to be available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fasttext_models = load_fasttext_models(\n",
    "    FASTTEXT_MODEL_DIR, languages=LANGUAGES, code_lookup=FASTTEXT_LANGUAGE_CODES\n",
    ")\n",
    "print(f\"Loaded fastText models for: {', '.join(sorted(fasttext_models))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Train the fastText baseline on Wikipedia (ID)\n",
    "\n",
    "We train a multinomial logistic regression classifier on averaged fastText embeddings derived from the Wikipedia training split and evaluate on the held-out Wikipedia test split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fasttext_clf, train_features = train_fasttext_classifier(\n",
    "    train_df.text.tolist(), train_df.label.tolist(), fasttext_models\n",
    ")\n",
    "\n",
    "id_eval = evaluate_fasttext_classifier(\n",
    "    fasttext_clf, test_df.text.tolist(), test_df.label.tolist(), fasttext_models\n",
    ")\n",
    "\n",
    "print(f\"In-distribution accuracy: {id_eval['accuracy']:.4f}\")\n",
    "print(json.dumps(id_eval[\"report\"], indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Evaluate on hate-speech/social-media corpora (OOD)\n\nThe classifier trained on Wikipedia data is tested on the held-out hate-speech/social-media corpora. Because each corpus is monolingual, the `language_hint` forces the matching fastText model for embedding extraction, revealing how poorly the Wikipedia-trained embeddings transfer for each target language."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ood_evals: Dict[str, Dict[str, object]] = {}\nfor lang, df in sorted(ood_sets.items()):\n    ood_eval = evaluate_fasttext_classifier(\n        fasttext_clf,\n        df.text.tolist(),\n        df.label.tolist(),\n        fasttext_models,\n        language_hint=lang,\n    )\n    ood_evals[lang] = ood_eval\n    print(f\"OOD accuracy ({lang} hate speech/social media): {ood_eval['accuracy']:.4f}\")\n    print(json.dumps(ood_eval[\"report\"], indent=2))\n\n# Combined OOD macro view without language hints to show overall transfer gaps\ncombined_ood_eval = evaluate_fasttext_classifier(\n    fasttext_clf, ood_df.text.tolist(), ood_df.label.tolist(), fasttext_models\n)\nprint(f\"Macro OOD accuracy across languages: {combined_ood_eval['accuracy']:.4f}\")\nprint(json.dumps(combined_ood_eval[\"report\"], indent=2))\n\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Quantitative comparison with Milestone 2 baselines\n",
    "\n",
    "Populate the baseline metrics below if you have already run the character n-gram TF\u2013IDF and XLM-R experiments. The performance drop column highlights how strongly each approach degrades under domain shift."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MILESTONE_TFIDF_ID = 0.9677  # Reported in Milestone 2\n",
    "MILESTONE_TFIDF_OOD = np.nan  # Replace with your measured OOD accuracy\n",
    "XLMR_ID = np.nan  # Replace with XLM-R in-distribution accuracy\n",
    "XLMR_OOD = np.nan  # Replace with XLM-R OOD accuracy\n",
    "\n",
    "comparison = pd.DataFrame(\n",
    "    {\n",
    "        \"Method\": [\"Char n-gram TF-IDF (Milestone 2)\", \"fastText embeddings\", \"XLM-R fine-tuning\"],\n",
    "        \"Wikipedia (ID) Accuracy\": [MILESTONE_TFIDF_ID, id_eval[\"accuracy\"], XLMR_ID],\n",
    "        \"Hate Speech (OOD) Accuracy\": [MILESTONE_TFIDF_OOD, ood_eval[\"accuracy\"], XLMR_OOD],\n",
    "    }\n",
    ")\n",
    "comparison[\"Performance Drop\"] = comparison[\"Wikipedia (ID) Accuracy\"] - comparison[\"Hate Speech (OOD) Accuracy\"]\n",
    "comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Error analysis\n\nWe compute OOV rates for Wikipedia vs. the OOD corpora, examine vocabulary overlap by language, and capture a sample of misclassified OOD examples to understand failure modes such as slang, code-switching, and short utterances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "oov_rows: List[Dict[str, object]] = []\nfor lang, model in fasttext_models.items():\n    wiki_subset = test_df[test_df.label == lang]\n    if wiki_subset.empty:\n        continue\n\n    wiki_oov = calculate_oov_rate(wiki_subset.text.tolist(), model)\n    ood_subset = ood_sets.get(lang)\n    ood_oov = calculate_oov_rate(ood_subset.text.tolist(), model) if ood_subset is not None else np.nan\n\n    train_subset = train_df[train_df.label == lang]\n    wiki_vocab = set(\" \".join(train_subset.text.tolist()).split())\n    ood_vocab = set(\" \".join(ood_subset.text.tolist()).split()) if ood_subset is not None else set()\n    vocab_overlap = len(wiki_vocab & ood_vocab) / max(len(ood_vocab), 1) if ood_vocab else np.nan\n\n    oov_rows.append(\n        {\n            \"language\": lang,\n            \"wiki_oov_rate\": wiki_oov,\n            \"ood_oov_rate\": ood_oov,\n            \"vocab_overlap\": vocab_overlap,\n            \"ood_unique_terms\": len(ood_vocab - wiki_vocab) if ood_vocab else np.nan,\n        }\n    )\n\nsummary_df = pd.DataFrame(oov_rows)\nprint(summary_df)\n\nerror_frames: List[pd.DataFrame] = []\nfor lang, eval_result in ood_evals.items():\n    errors = collect_misclassifications(\n        ood_sets[lang].text.tolist(),\n        ood_sets[lang].label.tolist(),\n        eval_result[\"predictions\"],\n        limit=20,\n    ).assign(language=lang)\n    error_frames.append(errors)\n\nerror_df = pd.concat(error_frames, ignore_index=True) if error_frames else pd.DataFrame()\nerror_df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Takeaways\n",
    "\n",
    "- fastText embeddings trained on Wikipedia/Common Crawl are sensitive to domain and vocabulary shift; expect lower accuracy on OOD hate-speech content than on in-distribution Wikipedia text.\n",
    "- Character n-gram TF-IDF baselines are often more robust to slang, profanity, and code-switching because they do not depend on a fixed vocabulary.\n",
    "- Manual inspection of OOV-heavy errors highlights how domain-specific slang and transliteration variants can break pretrained embeddings."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}