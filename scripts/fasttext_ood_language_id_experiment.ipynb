{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# fastText OOD Language Identification Experiment\n",
    "\n",
    "This notebook follows the Question 1 guidance by evaluating pretrained fastText embeddings on five languages: Kazakh, Latvian, Swedish, Yoruba, and Urdu. Wikipedia-derived CoNLL-U data are used for Latvian, Swedish, Yoruba, and Urdu, while the OOD Kazakh hate-speech corpus is drawn from `data/kazakh_hate_speech_fasttext.csv`. The goal is to illustrate how relying on pretrained fastText embeddings for language identification can break when confronting non-Wikipedia, out-of-distribution content."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup\n",
    "\n",
    "The notebook expects:\n",
    "\n",
    "- Wikipedia-derived CoNLL-U files for Latvian, Swedish, Yoruba, and Urdu under `data/<lang>/*.conllu`.\n",
    "- An OOD Kazakh hate-speech CSV file at `data/kazakh_hate_speech_fasttext.csv` with columns `text` and `label`.\n",
    "- Pretrained fastText binary models saved as `cc.<lang>.300.bin` in `models/fasttext/` (or adjust the paths below). Models are provided for all five target languages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "import json\n",
    "import random\n",
    "import warnings\n",
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "from typing import Dict, Iterable, List, Optional, Sequence, Tuple\n",
    "\n",
    "import fasttext\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reproducibility settings\n",
    "RANDOM_SEED = 13\n",
    "np.random.seed(RANDOM_SEED)\n",
    "random.seed(RANDOM_SEED)\n",
    "\n",
    "# Resolve project paths regardless of where the notebook is executed\n",
    "if \"__file__\" in globals():\n",
    "    _current_dir = Path(__file__).resolve().parent\n",
    "else:\n",
    "    _current_dir = Path.cwd().resolve()\n",
    "\n",
    "_possible_roots = [_current_dir, _current_dir.parent, _current_dir.parent.parent]\n",
    "PROJECT_ROOT = next((p for p in _possible_roots if (p / \"data\").exists()), None)\n",
    "if PROJECT_ROOT is None:\n",
    "    raise FileNotFoundError(\n",
    "        \"Could not locate the 'data' directory. Please run the notebook from the repository or ensure data is available.\"\n",
    "    )\n",
    "\n",
    "DATA_DIR = PROJECT_ROOT / \"data\"\n",
    "FASTTEXT_MODEL_DIR = PROJECT_ROOT / \"models\" / \"fasttext\"\n",
    "\n",
    "# Languages included in the Wikipedia dataset\n",
    "WIKI_LANGUAGES = [\"latvian\", \"swedish\", \"yoruba\", \"urdu\"]\n",
    "OOD_LANGUAGE = \"kazakh\"\n",
    "LANGUAGES = WIKI_LANGUAGES + [OOD_LANGUAGE]\n",
    "\n",
    "FASTTEXT_LANGUAGE_CODES: Dict[str, str] = {\n",
    "    \"kazakh\": \"kk\",\n",
    "    \"latvian\": \"lv\",\n",
    "    \"swedish\": \"sv\",\n",
    "    \"yoruba\": \"yo\",\n",
    "    \"urdu\": \"ur\",\n",
    "}\n",
    "\n",
    "# Optional: cap the number of sentences per language to keep the notebook fast\n",
    "MAX_SENTENCES_PER_LANGUAGE: Optional[int] = 2000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data loading helpers\n",
    "\n",
    "We reuse the Milestone 2 preprocessing assumptions: Wikipedia sentences are stored in CoNLL-U format with a `# text = ...` field. The hate-speech corpus is a simple CSV. Language labels are derived from the parent directory names for the Wikipedia data and set to `kazakh` for the OOD set to test language identification robustness."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class SentenceExample:\n",
    "    text: str\n",
    "    label: str\n",
    "\n",
    "\n",
    "def iter_conllu_sentences(path: Path) -> Iterable[str]:\n",
    "    \"\"\"Yield raw sentence strings from a CoNLL-U file.\"\"\"\n",
    "\n",
    "    buffer: List[str] = []\n",
    "    for line in path.read_text(encoding=\"utf8\").splitlines():\n",
    "        if line.startswith(\"# text = \"):\n",
    "            buffer.append(line[len(\"# text = \") :])\n",
    "        elif line.startswith(\"#\"):\n",
    "            continue\n",
    "        elif not line.strip():\n",
    "            if buffer:\n",
    "                yield \" \".join(buffer).strip()\n",
    "                buffer = []\n",
    "        else:\n",
    "            continue\n",
    "    if buffer:\n",
    "        yield \" \".join(buffer).strip()\n",
    "\n",
    "\n",
    "def load_multilingual_wikipedia(\n",
    "    data_root: Path,\n",
    "    languages: Sequence[str],\n",
    "    max_sentences_per_language: Optional[int] = None,\n",
    "    seed: int = RANDOM_SEED,\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"Load Wikipedia sentences and language labels into a DataFrame.\"\"\"\n",
    "\n",
    "    rng = random.Random(seed)\n",
    "    examples: List[SentenceExample] = []\n",
    "\n",
    "    for lang in sorted(languages):\n",
    "        lang_dir = data_root / lang\n",
    "        conllu_files = sorted(lang_dir.glob(\"*.conllu\"))\n",
    "        if not conllu_files:\n",
    "            warnings.warn(f\"No CoNLL-U files found for language: {lang}\")\n",
    "            continue\n",
    "        sentences: List[str] = []\n",
    "        for conllu in conllu_files:\n",
    "            sentences.extend(iter_conllu_sentences(conllu))\n",
    "        if max_sentences_per_language is not None:\n",
    "            rng.shuffle(sentences)\n",
    "            sentences = sentences[:max_sentences_per_language]\n",
    "        examples.extend(SentenceExample(text=s, label=lang) for s in sentences)\n",
    "\n",
    "    rng.shuffle(examples)\n",
    "    if not examples:\n",
    "        raise ValueError(\n",
    "            \"No Wikipedia sentences were loaded. Ensure data/<lang>/*.conllu files exist for the selected languages.\"\n",
    "        )\n",
    "    return pd.DataFrame([e.__dict__ for e in examples])\n",
    "\n",
    "\n",
    "def load_kazakh_hate_speech(path: Path) -> pd.DataFrame:\n",
    "    \"\"\"Load the OOD Kazakh hate-speech dataset.\"\"\"\n",
    "\n",
    "    df = pd.read_csv(path)\n",
    "    if \"text\" not in df.columns:\n",
    "        raise ValueError(\"Expected a 'text' column in the hate-speech CSV.\")\n",
    "    df = df.rename(columns={\"label\": \"hate_label\"})\n",
    "    df[\"label\"] = \"kazakh\"\n",
    "    return df[[\"text\", \"label\", \"hate_label\"]]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. fastText utilities\n",
    "\n",
    "The helpers below load language-specific fastText models, convert sentences to averaged word vectors, and compute out-of-vocabulary (OOV) rates for qualitative error analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_fasttext_models(\n",
    "    model_dir: Path, languages: Sequence[str], code_lookup: Optional[Dict[str, str]] = None\n",
    ") -> Dict[str, fasttext.FastText._FastText]:\n",
    "    \"\"\"Load fastText models for the specified languages.\n",
    "\n",
    "    The function expects files named `cc.<lang>.300.bin` inside `model_dir`. If a\n",
    "    model is missing, a warning is emitted and the language is skipped.\n",
    "    \"\"\"\n",
    "\n",
    "    code_lookup = code_lookup or {}\n",
    "    models: Dict[str, fasttext.FastText._FastText] = {}\n",
    "    for lang in languages:\n",
    "        code = code_lookup.get(lang, lang[:2])\n",
    "        path = model_dir / f\"cc.{code}.300.bin\"\n",
    "        if not path.exists():\n",
    "            warnings.warn(f\"Missing fastText model: {path}\")\n",
    "            continue\n",
    "        models[lang] = fasttext.load_model(path.as_posix())\n",
    "    if not models:\n",
    "        raise FileNotFoundError(\"No fastText models were loaded. Please download cc.<lang>.300.bin files.\")\n",
    "    return models\n",
    "\n",
    "\n",
    "def get_sentence_embedding(text: str, model: fasttext.FastText._FastText) -> np.ndarray:\n",
    "    \"\"\"Compute a sentence embedding by averaging token vectors.\"\"\"\n",
    "\n",
    "    tokens = text.split()\n",
    "    if not tokens:\n",
    "        return np.zeros(model.get_dimension(), dtype=np.float32)\n",
    "    vectors: List[np.ndarray] = [model.get_word_vector(tok) for tok in tokens]\n",
    "    return np.mean(vectors, axis=0)\n",
    "\n",
    "\n",
    "def extract_fasttext_features(\n",
    "    texts: Sequence[str],\n",
    "    models: Dict[str, fasttext.FastText._FastText],\n",
    "    language_hint: Optional[str] = None,\n",
    ") -> np.ndarray:\n",
    "    \"\"\"Convert sentences to feature matrices using language-specific models.\n",
    "\n",
    "    If `language_hint` is provided and exists in the model lookup, the\n",
    "    corresponding model is used for all texts (useful for OOD Kazakh-only\n",
    "    evaluation). Otherwise the first available model is used as a fallback.\n",
    "    \"\"\"\n",
    "\n",
    "    if language_hint and language_hint in models:\n",
    "        default_model = models[language_hint]\n",
    "    else:\n",
    "        default_model = models[sorted(models.keys())[0]]\n",
    "\n",
    "    features: List[np.ndarray] = []\n",
    "    for text in texts:\n",
    "        model = models.get(language_hint, default_model)\n",
    "        features.append(get_sentence_embedding(text, model))\n",
    "    return np.vstack(features)\n",
    "\n",
    "\n",
    "def is_in_vocabulary(word: str, model: fasttext.FastText._FastText) -> bool:\n",
    "    return model.get_word_id(word) != -1\n",
    "\n",
    "\n",
    "def calculate_oov_rate(texts: Sequence[str], model: fasttext.FastText._FastText) -> float:\n",
    "    \"\"\"Compute the average proportion of OOV tokens per sentence.\"\"\"\n",
    "\n",
    "    rates: List[float] = []\n",
    "    for text in texts:\n",
    "        tokens = text.split()\n",
    "        if not tokens:\n",
    "            rates.append(0.0)\n",
    "            continue\n",
    "        oov = sum(1 for tok in tokens if not is_in_vocabulary(tok, model))\n",
    "        rates.append(oov / len(tokens))\n",
    "    return float(np.mean(rates))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Model training and evaluation helpers\n",
    "\n",
    "We train a multinomial logistic regression classifier on averaged fastText embeddings and report accuracy, per-class precision/recall/F1, and confusion matrices. Additional utilities collect misclassified samples for manual inspection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_fasttext_classifier(\n",
    "    train_texts: Sequence[str],\n",
    "    train_labels: Sequence[str],\n",
    "    models: Dict[str, fasttext.FastText._FastText],\n",
    "):\n",
    "    features = extract_fasttext_features(train_texts, models)\n",
    "    clf = LogisticRegression(max_iter=1000, multi_class=\"multinomial\", solver=\"lbfgs\")\n",
    "    clf.fit(features, train_labels)\n",
    "    return clf, features\n",
    "\n",
    "\n",
    "def evaluate_fasttext_classifier(\n",
    "    clf: LogisticRegression,\n",
    "    texts: Sequence[str],\n",
    "    labels: Sequence[str],\n",
    "    models: Dict[str, fasttext.FastText._FastText],\n",
    "    language_hint: Optional[str] = None,\n",
    "):\n",
    "    features = extract_fasttext_features(texts, models, language_hint=language_hint)\n",
    "    preds = clf.predict(features)\n",
    "    acc = accuracy_score(labels, preds)\n",
    "    report = classification_report(labels, preds, output_dict=True, zero_division=0)\n",
    "    cm = confusion_matrix(labels, preds, labels=sorted(set(labels) | set(preds)))\n",
    "    return {\"accuracy\": acc, \"report\": report, \"confusion_matrix\": cm, \"predictions\": preds}\n",
    "\n",
    "\n",
    "def collect_misclassifications(\n",
    "    texts: Sequence[str],\n",
    "    labels: Sequence[str],\n",
    "    preds: Sequence[str],\n",
    "    limit: int = 20,\n",
    ") -> pd.DataFrame:\n",
    "    indices = [i for i, (y, p) in enumerate(zip(labels, preds)) if y != p]\n",
    "    sampled = indices[:limit]\n",
    "    return pd.DataFrame(\n",
    "        {\n",
    "            \"text\": [texts[i] for i in sampled],\n",
    "            \"true_label\": [labels[i] for i in sampled],\n",
    "            \"predicted_label\": [preds[i] for i in sampled],\n",
    "            \"token_count\": [len(texts[i].split()) for i in sampled],\n",
    "        }\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Load datasets\n",
    "\n",
    "The next cell loads the Wikipedia in-distribution (ID) data for Latvian, Swedish, Yoruba, and Urdu, then performs a reproducible train/validation split. It also loads the Kazakh hate-speech OOD data to stress-test language identification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wiki_df = load_multilingual_wikipedia(\n",
    "    DATA_DIR,\n",
    "    languages=WIKI_LANGUAGES,\n",
    "    max_sentences_per_language=MAX_SENTENCES_PER_LANGUAGE,\n",
    "    seed=RANDOM_SEED,\n",
    ")\n",
    "print(f\"Loaded {len(wiki_df)} Wikipedia sentences across {wiki_df.label.nunique()} languages\")\n",
    "\n",
    "train_df, test_df = train_test_split(\n",
    "    wiki_df, test_size=0.2, random_state=RANDOM_SEED, stratify=wiki_df.label\n",
    ")\n",
    "print(f\"Train size: {len(train_df)}, Test size: {len(test_df)}\")\n",
    "\n",
    "hate_df = load_kazakh_hate_speech(DATA_DIR / \"kazakh_hate_speech_fasttext.csv\")\n",
    "print(f\"Loaded {len(hate_df)} Kazakh hate-speech sentences\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Load pretrained fastText models\n",
    "\n",
    "Download the `cc.<lang>.300.bin` files from the [fastText](https://fasttext.cc/docs/en/crawl-vectors.html) repository and place them in `models/fasttext/` before running this cell. All five languages used in the experiment need to be available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fasttext_models = load_fasttext_models(\n",
    "    FASTTEXT_MODEL_DIR, languages=LANGUAGES, code_lookup=FASTTEXT_LANGUAGE_CODES\n",
    ")\n",
    "print(f\"Loaded fastText models for: {', '.join(sorted(fasttext_models))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Train the fastText baseline on Wikipedia (ID)\n",
    "\n",
    "We train a multinomial logistic regression classifier on averaged fastText embeddings derived from the Wikipedia training split and evaluate on the held-out Wikipedia test split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fasttext_clf, train_features = train_fasttext_classifier(\n",
    "    train_df.text.tolist(), train_df.label.tolist(), fasttext_models\n",
    ")\n",
    "\n",
    "id_eval = evaluate_fasttext_classifier(\n",
    "    fasttext_clf, test_df.text.tolist(), test_df.label.tolist(), fasttext_models\n",
    ")\n",
    "\n",
    "print(f\"In-distribution accuracy: {id_eval['accuracy']:.4f}\")\n",
    "print(json.dumps(id_eval[\"report\"], indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Evaluate on Kazakh hate-speech (OOD)\n",
    "\n",
    "The classifier trained on Wikipedia data is tested on the OOD hate-speech corpus. Because all texts are Kazakh, the `language_hint` forces the Kazakh fastText model for embedding extraction, revealing how poorly the Wikipedia-trained embeddings transfer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ood_eval = evaluate_fasttext_classifier(\n",
    "    fasttext_clf,\n",
    "    hate_df.text.tolist(),\n",
    "    hate_df.label.tolist(),\n",
    "    fasttext_models,\n",
    "    language_hint=\"kazakh\",\n",
    ")\n",
    "\n",
    "print(f\"OOD accuracy (Kazakh hate speech): {ood_eval['accuracy']:.4f}\")\n",
    "print(json.dumps(ood_eval[\"report\"], indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Quantitative comparison with Milestone 2 baselines\n",
    "\n",
    "Populate the baseline metrics below if you have already run the character n-gram TF\u2013IDF and XLM-R experiments. The performance drop column highlights how strongly each approach degrades under domain shift."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MILESTONE_TFIDF_ID = 0.9677  # Reported in Milestone 2\n",
    "MILESTONE_TFIDF_OOD = np.nan  # Replace with your measured OOD accuracy\n",
    "XLMR_ID = np.nan  # Replace with XLM-R in-distribution accuracy\n",
    "XLMR_OOD = np.nan  # Replace with XLM-R OOD accuracy\n",
    "\n",
    "comparison = pd.DataFrame(\n",
    "    {\n",
    "        \"Method\": [\"Char n-gram TF-IDF (Milestone 2)\", \"fastText embeddings\", \"XLM-R fine-tuning\"],\n",
    "        \"Wikipedia (ID) Accuracy\": [MILESTONE_TFIDF_ID, id_eval[\"accuracy\"], XLMR_ID],\n",
    "        \"Hate Speech (OOD) Accuracy\": [MILESTONE_TFIDF_OOD, ood_eval[\"accuracy\"], XLMR_OOD],\n",
    "    }\n",
    ")\n",
    "comparison[\"Performance Drop\"] = comparison[\"Wikipedia (ID) Accuracy\"] - comparison[\"Hate Speech (OOD) Accuracy\"]\n",
    "comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Error analysis\n",
    "\n",
    "We compute OOV rates for Wikipedia vs. hate-speech data, examine vocabulary overlap, and capture a sample of misclassified OOD examples to understand failure modes such as slang, code-switching, and short utterances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kk_model = fasttext_models.get(\"kazakh\") or next(iter(fasttext_models.values()))\n",
    "\n",
    "oov_wiki = calculate_oov_rate(test_df.text.tolist(), kk_model)\n",
    "oov_hate = calculate_oov_rate(hate_df.text.tolist(), kk_model)\n",
    "print(f\"OOV rate on Wikipedia test split: {oov_wiki:.2%}\")\n",
    "print(f\"OOV rate on hate-speech corpus: {oov_hate:.2%}\")\n",
    "\n",
    "wiki_vocab = set(\" \".join(train_df.text.tolist()).split())\n",
    "hate_vocab = set(\" \".join(hate_df.text.tolist()).split())\n",
    "vocab_overlap = len(wiki_vocab & hate_vocab) / max(len(hate_vocab), 1)\n",
    "print(f\"Vocabulary overlap (hate-speech vs. Wikipedia): {vocab_overlap:.2%}\")\n",
    "\n",
    "hate_only = sorted(hate_vocab - wiki_vocab)\n",
    "print(f\"Hate-speech-specific vocabulary items: {len(hate_only)}\")\n",
    "print(\"Sample:\", hate_only[:20])\n",
    "\n",
    "error_df = collect_misclassifications(\n",
    "    hate_df.text.tolist(), hate_df.label.tolist(), ood_eval[\"predictions\"], limit=20\n",
    ")\n",
    "error_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Takeaways\n",
    "\n",
    "- fastText embeddings trained on Wikipedia/Common Crawl are sensitive to domain and vocabulary shift; expect lower accuracy on OOD hate-speech content than on in-distribution Wikipedia text.\n",
    "- Character n-gram TF-IDF baselines are often more robust to slang, profanity, and code-switching because they do not depend on a fixed vocabulary.\n",
    "- Manual inspection of OOV-heavy errors highlights how domain-specific slang and transliteration variants can break pretrained embeddings."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}