{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8289644f",
   "metadata": {},
   "source": [
    "# fastText probing notebook\n\nThis notebook operationalises the experiments sketched in `reports/fasttext_limitations_and_kazakh.md`. It focuses on evaluating pretrained fastText vectors on non-English, out-of-distribution data\u2014specifically the Kazakh hate speech dataset stored at `data/kazakh_hate_speech_fasttext.csv`\u2014to interrogate their suitability as baseline features. It provides a reproducible pipeline for:\n\n1. Loading the Kazakh hate speech dataset (with optional multilingual Wikipedia fallback).\n2. Wiring in pretrained fastText vectors (e.g., the Kazakh `cc.kk.300.bin` model) to create sentence-level embeddings.\n3. Training a simple logistic regression classifier on averaged fastText vectors.\n4. Reporting held-out performance and surfacing misclassified examples for error analysis.\n\n> **Environment note:** The report mentions that package/model downloads were blocked in the grading environment. The notebook therefore detects whether fastText vectors are present locally and explains how to add them if they are missing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b3a0b9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import importlib.util\n",
    "import json\n",
    "import random\n",
    "from collections import Counter\n",
    "from dataclasses import dataclass\n",
    "from typing import Iterable, List, Optional, Sequence\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "FASTTEXT_AVAILABLE = importlib.util.find_spec('fasttext') is not None\n",
    "DATASETS_AVAILABLE = importlib.util.find_spec('datasets') is not None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b78fa27b",
   "metadata": {},
   "source": [
    "## Configure data and vectors\n\nSet the paths for the hate speech dataset and pretrained fastText vectors. The defaults expect the Kazakh hate speech CSV shipped with this repository and a local copy of the Kazakh vectors. Replace the paths with other language models (e.g., Yoruba) as needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3d4273e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fastText installed: True\n",
      "Vector file present: True (C:\\Users\\Maxim\\vectors\\cc.kk.300.bin)\n",
      "Data root resolved to: C:\\Users\\Maxim\\data (exists: True)\n"
     ]
    }
   ],
   "source": [
    "PROJECT_ROOT = Path.cwd()\nif (PROJECT_ROOT / 'data').is_dir():\n    DATA_ROOT = PROJECT_ROOT / 'data'\nelif (PROJECT_ROOT.parent / 'data').is_dir():\n    DATA_ROOT = PROJECT_ROOT.parent / 'data'\nelse:\n    DATA_ROOT = Path('data')\n\nLANGUAGES = ['kazakh stanza', 'yoruba heuristic', 'english heuristic']  # used for optional Wikipedia fallback\nKAZAKH_HATE_SPEECH_PATH = DATA_ROOT / 'kazakh_hate_speech_fasttext.csv'\nFASTTEXT_VECTOR_PATH = PROJECT_ROOT / 'vectors/cc.kk.300.bin'  # update if you store vectors elsewhere\n\nprint(f'fastText installed: {FASTTEXT_AVAILABLE}')\nprint(f'Vector file present: {FASTTEXT_VECTOR_PATH.exists()} ({FASTTEXT_VECTOR_PATH})')\nprint(f'Hate speech dataset present: {KAZAKH_HATE_SPEECH_PATH.exists()} ({KAZAKH_HATE_SPEECH_PATH})')\nprint(f'Data root resolved to: {DATA_ROOT} (exists: {DATA_ROOT.exists()})')\nif not FASTTEXT_AVAILABLE:\n    print('Set AUTO_INSTALL_FASTTEXT=True to let the notebook try installing the package via pip.')\nif not FASTTEXT_VECTOR_PATH.exists():\n    print('Set AUTO_DOWNLOAD_VECTORS=True to fetch the Kazakh fastText vectors automatically (large download).')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kazakh hate speech configuration\n\nToggle the switches below to prioritise the Kazakh hate speech corpus over Wikipedia snippets. The CSV lives in `data/kazakh_hate_speech_fasttext.csv` and provides non-Wikipedia, non-English content for probing pretrained vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "USE_KAZAKH_HATE_SPEECH = True  # enable to prioritise the hate speech corpus over Wikipedia snippets\nMAX_WIKIPEDIA_SENTENCES_PER_LANGUAGE = 2000  # fallback sample size per language when using Wikipedia data\n\nAUTO_INSTALL_FASTTEXT = True  # toggle to attempt `pip install fasttext-wheel` (set False to skip installs)\nAUTO_DOWNLOAD_VECTORS = True  # toggle to download cc.kk.300.bin (~1.2GB); set False if you already have the file or are offline\nFASTTEXT_INSTALL_PACKAGE = 'fasttext-wheel'\nFASTTEXT_DOWNLOAD_URL = 'https://dl.fbaipublicfiles.com/fasttext/vectors-crawl/cc.kk.300.bin.gz'\n\ndef ensure_fasttext_installed(auto_install: bool = False) -> bool:\n    global FASTTEXT_AVAILABLE\n    if FASTTEXT_AVAILABLE:\n        return True\n    if not auto_install:\n        print(\n            'fastText Python bindings are not installed. Install via `pip install fasttext-wheel` '\n            'or set AUTO_INSTALL_FASTTEXT=True to let the notebook attempt installation.'\n        )\n        return False\n    try:\n        import subprocess\n        import sys\n\n        subprocess.check_call([sys.executable, '-m', 'pip', 'install', FASTTEXT_INSTALL_PACKAGE])\n        FASTTEXT_AVAILABLE = importlib.util.find_spec('fasttext') is not None\n    except Exception as exc:  # noqa: BLE001\n        print(f'Automatic installation failed: {exc}')\n        FASTTEXT_AVAILABLE = False\n    return FASTTEXT_AVAILABLE\n\ndef download_fasttext_vectors(target_path: Path, url: str) -> bool:\n    import gzip\n    import shutil\n    import urllib.request\n\n    target_path.parent.mkdir(parents=True, exist_ok=True)\n    gz_path = target_path.with_suffix(target_path.suffix + '.gz')\n    try:\n        with urllib.request.urlopen(url) as resp, open(gz_path, 'wb') as download_out:\n            shutil.copyfileobj(resp, download_out)\n        with gzip.open(gz_path, 'rb') as src, open(target_path, 'wb') as dst:\n            shutil.copyfileobj(src, dst)\n        return True\n    except Exception as exc:  # noqa: BLE001\n        print(f'Downloading fastText vectors failed: {exc}')\n        return False\n    finally:\n        if gz_path.exists():\n            try:\n                gz_path.unlink()\n            except OSError:\n                pass\n\ndef ensure_vector_file(target_path: Path, url: str, auto_download: bool = False) -> bool:\n    if target_path.exists():\n        return True\n    if not auto_download:\n        print(\n            f'Vector file not found at {target_path}. '\n            'Set AUTO_DOWNLOAD_VECTORS=True to download automatically or place it manually.'\n        )\n        return False\n    print(f'Downloading fastText vectors from {url} (this is ~1.2GB)...')\n    return download_fasttext_vectors(target_path, url)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "46e6b9f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "AUTO_INSTALL_FASTTEXT = True  # toggle to attempt `pip install fasttext-wheel` (set False to skip installs)\n",
    "AUTO_DOWNLOAD_VECTORS = True  # toggle to download cc.kk.300.bin (~1.2GB); set False if you already have the file or are offline\n",
    "FASTTEXT_INSTALL_PACKAGE = 'fasttext-wheel'\n",
    "FASTTEXT_DOWNLOAD_URL = 'https://dl.fbaipublicfiles.com/fasttext/vectors-crawl/cc.kk.300.bin.gz'\n",
    "\n",
    "def ensure_fasttext_installed(auto_install: bool = False) -> bool:\n",
    "    global FASTTEXT_AVAILABLE\n",
    "    if FASTTEXT_AVAILABLE:\n",
    "        return True\n",
    "    if not auto_install:\n",
    "        print(\n",
    "            'fastText Python bindings are not installed. Install via `pip install fasttext-wheel` '\n",
    "            'or set AUTO_INSTALL_FASTTEXT=True to let the notebook attempt installation.'\n",
    "        )\n",
    "        return False\n",
    "    try:\n",
    "        import subprocess\n",
    "        import sys\n",
    "\n",
    "        subprocess.check_call([sys.executable, '-m', 'pip', 'install', FASTTEXT_INSTALL_PACKAGE])\n",
    "        FASTTEXT_AVAILABLE = importlib.util.find_spec('fasttext') is not None\n",
    "    except Exception as exc:  # noqa: BLE001\n",
    "        print(f'Automatic installation failed: {exc}')\n",
    "        FASTTEXT_AVAILABLE = False\n",
    "    return FASTTEXT_AVAILABLE\n",
    "\n",
    "def download_fasttext_vectors(target_path: Path, url: str) -> bool:\n",
    "    import gzip\n",
    "    import shutil\n",
    "    import urllib.request\n",
    "\n",
    "    target_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    gz_path = target_path.with_suffix(target_path.suffix + '.gz')\n",
    "    try:\n",
    "        with urllib.request.urlopen(url) as resp, open(gz_path, 'wb') as download_out:\n",
    "            shutil.copyfileobj(resp, download_out)\n",
    "        with gzip.open(gz_path, 'rb') as src, open(target_path, 'wb') as dst:\n",
    "            shutil.copyfileobj(src, dst)\n",
    "        return True\n",
    "    except Exception as exc:  # noqa: BLE001\n",
    "        print(f'Downloading fastText vectors failed: {exc}')\n",
    "        return False\n",
    "    finally:\n",
    "        if gz_path.exists():\n",
    "            try:\n",
    "                gz_path.unlink()\n",
    "            except OSError:\n",
    "                pass\n",
    "\n",
    "def ensure_vector_file(target_path: Path, url: str, auto_download: bool = False) -> bool:\n",
    "    if target_path.exists():\n",
    "        return True\n",
    "    if not auto_download:\n",
    "        print(\n",
    "            f'Vector file not found at {target_path}. '\n",
    "            'Set AUTO_DOWNLOAD_VECTORS=True to download automatically or place it manually.'\n",
    "        )\n",
    "        return False\n",
    "    print(f'Downloading fastText vectors from {url} (this is ~1.2GB)...')\n",
    "    return download_fasttext_vectors(target_path, url)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce1609f0",
   "metadata": {},
   "source": [
    "## Data loading helpers\n\nThe helpers below mirror the logic used in the baseline scripts (`scripts/evaluate_language_id_baselines.py`) but trim it down for quick experimentation inside the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5d74c846",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                text              label\n",
      "0                           Itokasi \u00c0w\u1ecdn \u00e1st\u1eb9\u0301r\u1ecd\u0301\u00ecd\u00ec   yoruba heuristic\n",
      "1  Focusing on oneself is not listening, reading,...  english heuristic\n",
      "2                                                ...  english heuristic\n",
      "3  \u0422\u04e9\u0440\u0442 \u043c\u0435\u0437\u0433\u0456\u043b \u0442\u0430\u043c\u0430\u049b\u0442\u0430\u043d\u0443 \u0430\u043d\u0430\u0493\u04b1\u0440\u043b\u044b\u043c \u0442\u0438\u0456\u043c\u0434\u0456 \u0431\u043e\u043b\u044b\u043f \u0442...      kazakh stanza\n",
      "4  \u0416\u0430\u043c\u0431\u044b\u043b \u0433\u0438\u0434\u0440\u043e\u043c\u0435\u043b\u0438\u043e\u0440\u0430\u0442\u0438\u0432\u0442\u0456\u043a-\u049b\u04b1\u0440\u044b\u043b\u044b\u0441 \u0438\u043d\u0441\u0442\u0438\u0442\u0443\u0442\u044b\u043d (...      kazakh stanza\n",
      "Class distribution: yoruba heuristic     2000\n",
      "english heuristic    2000\n",
      "kazakh stanza        2000\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "@dataclass\nclass SentenceExample:\n    text: str\n    label: str\n\n\ndef iter_conllu_sentences(path: Path) -> Iterable[str]:\n    buffer: List[str] = []\n    for line in path.read_text(encoding='utf8').splitlines():\n        if line.startswith('# text = '):\n            buffer.append(line[len('# text = ') :])\n        elif line.startswith('#'):\n            continue\n        elif not line.strip():\n            if buffer:\n                yield ' '.join(buffer).strip()\n                buffer = []\n        else:\n            continue\n    if buffer:\n        yield ' '.join(buffer).strip()\n\n\ndef load_multilingual_dataset(\n    data_root: Path,\n    languages: Optional[Sequence[str]] = None,\n    max_sentences_per_language: Optional[int] = None,\n    seed: int = 13,\n) -> pd.DataFrame:\n    rng = random.Random(seed)\n    examples: List[SentenceExample] = []\n    language_dirs = sorted([p for p in data_root.iterdir() if p.is_dir()])\n    for lang_dir in language_dirs:\n        language = lang_dir.name\n        if languages and language not in languages:\n            continue\n        sentences: List[str] = []\n        for conllu_file in sorted(lang_dir.glob('*.conllu')):\n            sentences.extend(iter_conllu_sentences(conllu_file))\n        if max_sentences_per_language is not None:\n            rng.shuffle(sentences)\n            sentences = sentences[:max_sentences_per_language]\n        examples.extend(SentenceExample(text=s, label=language) for s in sentences)\n    rng.shuffle(examples)\n    return pd.DataFrame([vars(example) for example in examples], columns=['text', 'label'])\n\n\ndef load_kazakh_hate_speech_dataset(path: Path) -> pd.DataFrame:\n    if not path.exists():\n        print(f'Kazakh hate speech file not found at {path}. Place the CSV to prioritise this corpus.')\n        return pd.DataFrame(columns=['text', 'label'])\n    df = pd.read_csv(path)\n    missing = {'text', 'label'} - set(df.columns)\n    if missing:\n        print(\n            f'Expected columns `text` and `label` were not found in {path}. '\n            f'Missing: {sorted(missing)}'\n        )\n        return pd.DataFrame(columns=['text', 'label'])\n    df = df[['text', 'label']].dropna()\n    print(f'Loaded {len(df):,} Kazakh hate speech records from {path}.')\n    return df\n\n\ndef preview_class_balance(df: pd.DataFrame) -> pd.Series:\n    counts = Counter(df['label'])\n    return pd.Series(counts).sort_values(ascending=False)\n\n\nwikipedia_dataset = load_multilingual_dataset(\n    DATA_ROOT, languages=LANGUAGES, max_sentences_per_language=MAX_WIKIPEDIA_SENTENCES_PER_LANGUAGE\n)\nif wikipedia_dataset.empty:\n    raise RuntimeError(\n        'No sentences were loaded from DATA_ROOT. Ensure the corpus is available and LANGUAGES is set correctly.'\n    )\nprint('Wikipedia-derived sample:')\nprint(wikipedia_dataset.head())\nprint('Class distribution:', preview_class_balance(wikipedia_dataset))\n\nkazakh_hate_speech_df = load_kazakh_hate_speech_dataset(KAZAKH_HATE_SPEECH_PATH) if USE_KAZAKH_HATE_SPEECH else pd.DataFrame()\n\nif USE_KAZAKH_HATE_SPEECH and not kazakh_hate_speech_df.empty:\n    dataset = kazakh_hate_speech_df\n    print('Using Kazakh hate speech corpus (non-Wikipedia) for probing fastText embeddings.')\nelse:\n    dataset = wikipedia_dataset\n    print('Falling back to Wikipedia-derived multilingual dataset.')\nprint('Active dataset label balance:')\nprint(preview_class_balance(dataset))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cell above now swaps in the Kazakh hate speech corpus when available, ensuring the evaluation targets non-Wikipedia, non-English data. If the CSV is missing or misformatted, the notebook falls back to the Wikipedia-derived multilingual dataset for convenience."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "882bebea",
   "metadata": {},
   "source": [
    "## Load fastText vectors\n",
    "\n",
    "The cell below loads a local binary `.bin` file with subword vectors. If the file is missing or the `fasttext` package is unavailable, the notebook surfaces clear guidance on how to proceed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e9492cf1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded fastText model with 300 dimensions from C:\\Users\\Maxim\\vectors\\cc.kk.300.bin\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "fasttext_model = None\n",
    "fasttext_dim = None\n",
    "\n",
    "fasttext_ready = ensure_fasttext_installed(AUTO_INSTALL_FASTTEXT)\n",
    "vector_ready = ensure_vector_file(FASTTEXT_VECTOR_PATH, FASTTEXT_DOWNLOAD_URL, AUTO_DOWNLOAD_VECTORS)\n",
    "\n",
    "if fasttext_ready and vector_ready:\n",
    "    import fasttext  # type: ignore\n",
    "\n",
    "    fasttext_model = fasttext.load_model(str(FASTTEXT_VECTOR_PATH))\n",
    "    fasttext_dim = fasttext_model.get_dimension()\n",
    "    print(f'Loaded fastText model with {fasttext_dim} dimensions from {FASTTEXT_VECTOR_PATH}')\n",
    "else:\n",
    "    guidance = []\n",
    "    if not fasttext_ready:\n",
    "        guidance.append(\n",
    "            '- fastText Python bindings are missing. Set AUTO_INSTALL_FASTTEXT=True or install manually via `pip install fasttext-wheel`.'\n",
    "        )\n",
    "    if not vector_ready:\n",
    "        guidance.append(\n",
    "            f'- fastText vector binary not found at {FASTTEXT_VECTOR_PATH}. Set AUTO_DOWNLOAD_VECTORS=True to fetch it or place it manually.'\n",
    "        )\n",
    "    raise RuntimeError(\n",
    "        'fastText setup is incomplete; please address the items below before rerunning:\\n' + '\\n'.join(guidance)\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ecf3c16",
   "metadata": {},
   "source": [
    "## Feature construction and model training\n",
    "\n",
    "Sentences are tokenised on whitespace and averaged into a single embedding vector. This mirrors the lightweight fastText probing described in the report (averaging static vectors before a linear classifier)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "772b2548",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Maxim\\conda\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {\n",
       "  /* Definition of color scheme common for light and dark mode */\n",
       "  --sklearn-color-text: #000;\n",
       "  --sklearn-color-text-muted: #666;\n",
       "  --sklearn-color-line: gray;\n",
       "  /* Definition of color scheme for unfitted estimators */\n",
       "  --sklearn-color-unfitted-level-0: #fff5e6;\n",
       "  --sklearn-color-unfitted-level-1: #f6e4d2;\n",
       "  --sklearn-color-unfitted-level-2: #ffe0b3;\n",
       "  --sklearn-color-unfitted-level-3: chocolate;\n",
       "  /* Definition of color scheme for fitted estimators */\n",
       "  --sklearn-color-fitted-level-0: #f0f8ff;\n",
       "  --sklearn-color-fitted-level-1: #d4ebff;\n",
       "  --sklearn-color-fitted-level-2: #b3dbfd;\n",
       "  --sklearn-color-fitted-level-3: cornflowerblue;\n",
       "\n",
       "  /* Specific color for light theme */\n",
       "  --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, white)));\n",
       "  --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-icon: #696969;\n",
       "\n",
       "  @media (prefers-color-scheme: dark) {\n",
       "    /* Redefinition of color scheme for dark theme */\n",
       "    --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, #111)));\n",
       "    --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-icon: #878787;\n",
       "  }\n",
       "}\n",
       "\n",
       "#sk-container-id-1 {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 pre {\n",
       "  padding: 0;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 input.sk-hidden--visually {\n",
       "  border: 0;\n",
       "  clip: rect(1px 1px 1px 1px);\n",
       "  clip: rect(1px, 1px, 1px, 1px);\n",
       "  height: 1px;\n",
       "  margin: -1px;\n",
       "  overflow: hidden;\n",
       "  padding: 0;\n",
       "  position: absolute;\n",
       "  width: 1px;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-dashed-wrapped {\n",
       "  border: 1px dashed var(--sklearn-color-line);\n",
       "  margin: 0 0.4em 0.5em 0.4em;\n",
       "  box-sizing: border-box;\n",
       "  padding-bottom: 0.4em;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-container {\n",
       "  /* jupyter's `normalize.less` sets `[hidden] { display: none; }`\n",
       "     but bootstrap.min.css set `[hidden] { display: none !important; }`\n",
       "     so we also need the `!important` here to be able to override the\n",
       "     default hidden behavior on the sphinx rendered scikit-learn.org.\n",
       "     See: https://github.com/scikit-learn/scikit-learn/issues/21755 */\n",
       "  display: inline-block !important;\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-text-repr-fallback {\n",
       "  display: none;\n",
       "}\n",
       "\n",
       "div.sk-parallel-item,\n",
       "div.sk-serial,\n",
       "div.sk-item {\n",
       "  /* draw centered vertical line to link estimators */\n",
       "  background-image: linear-gradient(var(--sklearn-color-text-on-default-background), var(--sklearn-color-text-on-default-background));\n",
       "  background-size: 2px 100%;\n",
       "  background-repeat: no-repeat;\n",
       "  background-position: center center;\n",
       "}\n",
       "\n",
       "/* Parallel-specific style estimator block */\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item::after {\n",
       "  content: \"\";\n",
       "  width: 100%;\n",
       "  border-bottom: 2px solid var(--sklearn-color-text-on-default-background);\n",
       "  flex-grow: 1;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel {\n",
       "  display: flex;\n",
       "  align-items: stretch;\n",
       "  justify-content: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item:first-child::after {\n",
       "  align-self: flex-end;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item:last-child::after {\n",
       "  align-self: flex-start;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item:only-child::after {\n",
       "  width: 0;\n",
       "}\n",
       "\n",
       "/* Serial-specific style estimator block */\n",
       "\n",
       "#sk-container-id-1 div.sk-serial {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "  align-items: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  padding-right: 1em;\n",
       "  padding-left: 1em;\n",
       "}\n",
       "\n",
       "\n",
       "/* Toggleable style: style used for estimator/Pipeline/ColumnTransformer box that is\n",
       "clickable and can be expanded/collapsed.\n",
       "- Pipeline and ColumnTransformer use this feature and define the default style\n",
       "- Estimators will overwrite some part of the style using the `sk-estimator` class\n",
       "*/\n",
       "\n",
       "/* Pipeline and ColumnTransformer style (default) */\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable {\n",
       "  /* Default theme specific background. It is overwritten whether we have a\n",
       "  specific estimator or a Pipeline/ColumnTransformer */\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "/* Toggleable label */\n",
       "#sk-container-id-1 label.sk-toggleable__label {\n",
       "  cursor: pointer;\n",
       "  display: flex;\n",
       "  width: 100%;\n",
       "  margin-bottom: 0;\n",
       "  padding: 0.5em;\n",
       "  box-sizing: border-box;\n",
       "  text-align: center;\n",
       "  align-items: start;\n",
       "  justify-content: space-between;\n",
       "  gap: 0.5em;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 label.sk-toggleable__label .caption {\n",
       "  font-size: 0.6rem;\n",
       "  font-weight: lighter;\n",
       "  color: var(--sklearn-color-text-muted);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 label.sk-toggleable__label-arrow:before {\n",
       "  /* Arrow on the left of the label */\n",
       "  content: \"\u25b8\";\n",
       "  float: left;\n",
       "  margin-right: 0.25em;\n",
       "  color: var(--sklearn-color-icon);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "/* Toggleable content - dropdown */\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content {\n",
       "  max-height: 0;\n",
       "  max-width: 0;\n",
       "  overflow: hidden;\n",
       "  text-align: left;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content pre {\n",
       "  margin: 0.2em;\n",
       "  border-radius: 0.25em;\n",
       "  color: var(--sklearn-color-text);\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content.fitted pre {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {\n",
       "  /* Expand drop-down */\n",
       "  max-height: 200px;\n",
       "  max-width: 100%;\n",
       "  overflow: auto;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {\n",
       "  content: \"\u25be\";\n",
       "}\n",
       "\n",
       "/* Pipeline/ColumnTransformer-specific style */\n",
       "\n",
       "#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-label.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator-specific style */\n",
       "\n",
       "/* Colorize estimator box */\n",
       "#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-estimator.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-label label.sk-toggleable__label,\n",
       "#sk-container-id-1 div.sk-label label {\n",
       "  /* The background is the default theme color */\n",
       "  color: var(--sklearn-color-text-on-default-background);\n",
       "}\n",
       "\n",
       "/* On hover, darken the color of the background */\n",
       "#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "/* Label box, darken color on hover, fitted */\n",
       "#sk-container-id-1 div.sk-label.fitted:hover label.sk-toggleable__label.fitted {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator label */\n",
       "\n",
       "#sk-container-id-1 div.sk-label label {\n",
       "  font-family: monospace;\n",
       "  font-weight: bold;\n",
       "  display: inline-block;\n",
       "  line-height: 1.2em;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-label-container {\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       "/* Estimator-specific */\n",
       "#sk-container-id-1 div.sk-estimator {\n",
       "  font-family: monospace;\n",
       "  border: 1px dotted var(--sklearn-color-border-box);\n",
       "  border-radius: 0.25em;\n",
       "  box-sizing: border-box;\n",
       "  margin-bottom: 0.5em;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-estimator.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "/* on hover */\n",
       "#sk-container-id-1 div.sk-estimator:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-estimator.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Specification for estimator info (e.g. \"i\" and \"?\") */\n",
       "\n",
       "/* Common style for \"i\" and \"?\" */\n",
       "\n",
       ".sk-estimator-doc-link,\n",
       "a:link.sk-estimator-doc-link,\n",
       "a:visited.sk-estimator-doc-link {\n",
       "  float: right;\n",
       "  font-size: smaller;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1em;\n",
       "  height: 1em;\n",
       "  width: 1em;\n",
       "  text-decoration: none !important;\n",
       "  margin-left: 0.5em;\n",
       "  text-align: center;\n",
       "  /* unfitted */\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted,\n",
       "a:link.sk-estimator-doc-link.fitted,\n",
       "a:visited.sk-estimator-doc-link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "div.sk-estimator:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "div.sk-estimator.fitted:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "/* Span, style for the box shown on hovering the info icon */\n",
       ".sk-estimator-doc-link span {\n",
       "  display: none;\n",
       "  z-index: 9999;\n",
       "  position: relative;\n",
       "  font-weight: normal;\n",
       "  right: .2ex;\n",
       "  padding: .5ex;\n",
       "  margin: .5ex;\n",
       "  width: min-content;\n",
       "  min-width: 20ex;\n",
       "  max-width: 50ex;\n",
       "  color: var(--sklearn-color-text);\n",
       "  box-shadow: 2pt 2pt 4pt #999;\n",
       "  /* unfitted */\n",
       "  background: var(--sklearn-color-unfitted-level-0);\n",
       "  border: .5pt solid var(--sklearn-color-unfitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted span {\n",
       "  /* fitted */\n",
       "  background: var(--sklearn-color-fitted-level-0);\n",
       "  border: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link:hover span {\n",
       "  display: block;\n",
       "}\n",
       "\n",
       "/* \"?\"-specific style due to the `<a>` HTML tag */\n",
       "\n",
       "#sk-container-id-1 a.estimator_doc_link {\n",
       "  float: right;\n",
       "  font-size: 1rem;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1rem;\n",
       "  height: 1rem;\n",
       "  width: 1rem;\n",
       "  text-decoration: none;\n",
       "  /* unfitted */\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 a.estimator_doc_link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "#sk-container-id-1 a.estimator_doc_link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 a.estimator_doc_link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LogisticRegression(max_iter=1000, multi_class=&#x27;auto&#x27;, n_jobs=-1)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow\"><div><div>LogisticRegression</div></div><div><a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.6/modules/generated/sklearn.linear_model.LogisticRegression.html\">?<span>Documentation for LogisticRegression</span></a><span class=\"sk-estimator-doc-link fitted\">i<span>Fitted</span></span></div></label><div class=\"sk-toggleable__content fitted\"><pre>LogisticRegression(max_iter=1000, multi_class=&#x27;auto&#x27;, n_jobs=-1)</pre></div> </div></div></div></div>"
      ],
      "text/plain": [
       "LogisticRegression(max_iter=1000, multi_class='auto', n_jobs=-1)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def sentence_to_vector(text: str, model, dim: int) -> np.ndarray:\n",
    "    tokens = text.strip().split()\n",
    "    if not tokens:\n",
    "        return np.zeros(dim, dtype=np.float32)\n",
    "    vectors = [model.get_word_vector(tok) for tok in tokens]\n",
    "    return np.mean(np.stack(vectors, axis=0), axis=0)\n",
    "\n",
    "\n",
    "def build_embedding_matrix(texts: Sequence[str], model, dim: int) -> np.ndarray:\n",
    "    return np.vstack([sentence_to_vector(text, model, dim) for text in texts])\n",
    "\n",
    "\n",
    "if fasttext_model is None:\n",
    "    raise RuntimeError(\n",
    "        'A fastText model is required to continue. Resolve the setup issues above (installation or vector download) and rerun the loader cell.'\n",
    "    )\n",
    "\n",
    "X = build_embedding_matrix(dataset['text'], fasttext_model, fasttext_dim)\n",
    "y = dataset['label'].to_numpy()\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "classifier = LogisticRegression(max_iter=1000, n_jobs=-1, multi_class='auto')\n",
    "classifier.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "832725dd",
   "metadata": {},
   "source": [
    "## Evaluation and error inspection\n",
    "\n",
    "Accuracy and macro-averaged precision/recall/F1 provide a quick snapshot of how well fastText embeddings separate the selected languages. Misclassifications are shown to facilitate the qualitative inspection suggested in the report."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "adad9a43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy: 0.9483\n",
      "\n",
      "                   precision    recall  f1-score   support\n",
      "\n",
      "english heuristic       0.91      0.95      0.93       400\n",
      "    kazakh stanza       0.98      0.99      0.98       400\n",
      " yoruba heuristic       0.96      0.91      0.93       400\n",
      "\n",
      "         accuracy                           0.95      1200\n",
      "        macro avg       0.95      0.95      0.95      1200\n",
      "     weighted avg       0.95      0.95      0.95      1200\n",
      "\n",
      "Sample misclassifications (head):\n",
      "                                                text               gold  \\\n",
      "0                                                ...  english heuristic   \n",
      "1                                    \u0410\u0443\u0437\u044b- \u049b\u043e\u0441\u0436\u0430\u049b\u0442\u044b.      kazakh stanza   \n",
      "2                                      Heerlen, N.V.  english heuristic   \n",
      "3  Durkheim, Marx, and the German theorist Max We...   yoruba heuristic   \n",
      "4  Get to the fucking point.\" Brooks came into hi...   yoruba heuristic   \n",
      "\n",
      "                pred  \n",
      "0      kazakh stanza  \n",
      "1   yoruba heuristic  \n",
      "2   yoruba heuristic  \n",
      "3  english heuristic  \n",
      "4  english heuristic  \n"
     ]
    }
   ],
   "source": [
    "y_pred = classifier.predict(X_test)\n",
    "print(f'Test accuracy: {accuracy_score(y_test, y_pred):.4f}\\n')\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "errors = []\n",
    "for text, gold, pred in zip(dataset['text'], y, classifier.predict(X)):\n",
    "    if gold != pred:\n",
    "        errors.append({'text': text, 'gold': gold, 'pred': pred})\n",
    "\n",
    "error_df = pd.DataFrame(errors)\n",
    "print('Sample misclassifications (head):')\n",
    "print(error_df.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de220367",
   "metadata": {},
   "source": [
    "## Next steps\n",
    "\n",
    "* Swap in domain-specific corpora (e.g., Yoruba social media posts) by replacing the `DATA_ROOT` path or loading an external dataframe.\n",
    "* Point `FASTTEXT_VECTOR_PATH` at the matching pretrained vectors (such as `cc.kk.300.bin` for Kazakh or `cc.yo.300.bin` for Yoruba) to mirror the report's planned experiments.\n",
    "* Extend the analysis by saving confusion matrices or integrating alternative feature baselines (character n-grams) to quantify the gap between static embeddings and more robust representations."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}