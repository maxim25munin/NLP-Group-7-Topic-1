{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8289644f",
   "metadata": {},
   "source": [
    "# fastText probing notebook\n",
    "\n",
    "This notebook operationalises the experiments sketched in `reports/fasttext_limitations_and_kazakh.md`. It provides a reproducible pipeline for:\n",
    "\n",
    "1. Loading multilingual Wikipedia snippets from the repository's `data/` directory.\n",
    "2. Wiring in pretrained fastText vectors (e.g., the Kazakh `cc.kk.300.bin` model) to create sentence-level embeddings.\n",
    "3. Training a simple logistic regression classifier on averaged fastText vectors.\n",
    "4. Reporting held-out performance and surfacing misclassified examples for error analysis.\n",
    "\n",
    "> **Environment note:** The report mentions that package/model downloads were blocked in the grading environment. The notebook therefore detects whether fastText vectors are present locally and explains how to add them if they are missing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b3a0b9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import importlib.util\n",
    "import json\n",
    "import random\n",
    "from collections import Counter\n",
    "from dataclasses import dataclass\n",
    "from typing import Iterable, List, Optional, Sequence\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "FASTTEXT_AVAILABLE = importlib.util.find_spec('fasttext') is not None\n",
    "DATASETS_AVAILABLE = importlib.util.find_spec('datasets') is not None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b78fa27b",
   "metadata": {},
   "source": [
    "## Configure data and vectors\n",
    "\n",
    "Set the languages to evaluate and the location of your pretrained fastText vectors. The defaults work with the repository's Wikipedia-derived dataset and expect a local copy of the Kazakh vectors. Replace the paths with other language models (e.g., Yoruba) as needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3d4273e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fastText installed: True\n",
      "Vector file present: True (C:\\Users\\Maxim\\vectors\\cc.kk.300.bin)\n",
      "Data root resolved to: C:\\Users\\Maxim\\data (exists: True)\n"
     ]
    }
   ],
   "source": [
    "PROJECT_ROOT = Path.cwd()\n",
    "if (PROJECT_ROOT / 'data').is_dir():\n",
    "    DATA_ROOT = PROJECT_ROOT / 'data'\n",
    "elif (PROJECT_ROOT.parent / 'data').is_dir():\n",
    "    DATA_ROOT = PROJECT_ROOT.parent / 'data'\n",
    "else:\n",
    "    DATA_ROOT = Path('data')\n",
    "LANGUAGES = ['kazakh stanza', 'yoruba heuristic', 'english heuristic']  # adjust to probe different subsets\n",
    "FASTTEXT_VECTOR_PATH = PROJECT_ROOT / 'vectors/cc.kk.300.bin'  # update if you store vectors elsewhere\n",
    "\n",
    "print(f'fastText installed: {FASTTEXT_AVAILABLE}')\n",
    "print(f'Vector file present: {FASTTEXT_VECTOR_PATH.exists()} ({FASTTEXT_VECTOR_PATH})')\n",
    "print(f'Data root resolved to: {DATA_ROOT} (exists: {DATA_ROOT.exists()})')\n",
    "if not FASTTEXT_AVAILABLE:\n",
    "    print('Set AUTO_INSTALL_FASTTEXT=True to let the notebook try installing the package via pip.')\n",
    "if not FASTTEXT_VECTOR_PATH.exists():\n",
    "    print('Set AUTO_DOWNLOAD_VECTORS=True to fetch the Kazakh fastText vectors automatically (large download).')\n",
    "if not FASTTEXT_AVAILABLE or not FASTTEXT_VECTOR_PATH.exists():\n",
    "    print('The cells below now default to enabling both toggles so setup can continue automatically unless you opt out.')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kazakh sentiment configuration\n",
    "\n",
    "Toggle the switches below to automatically install the Hugging Face `datasets` library and fetch the Kazakh Sentiment Analysis Dataset of Reviews and Attitudes (KazSAnDRA). If you already have a local CSV export, point `KAZSANDRA_LOCAL_PATH` at it and disable auto-downloads.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "USE_KAZSANDRA = True  # enable to prioritise the Kazakh sentiment corpus over Wikipedia snippets\n",
    "AUTO_INSTALL_DATASETS = True  # install the `datasets` package if it is missing\n",
    "AUTO_DOWNLOAD_KAZSANDRA = True  # attempt to fetch the dataset from Hugging Face when no local CSV is present\n",
    "KAZSANDRA_LOCAL_PATH = PROJECT_ROOT / 'data' / 'kazsandra_full.csv'\n",
    "KAZSANDRA_HF_DATASET = 'issai/kazsandra'  # Hugging Face dataset identifier\n",
    "KAZSANDRA_HF_CONFIG = 'full'  # dataset configuration name (e.g., 'full')\n",
    "KAZSANDRA_HF_SPLIT = 'train'  # split to use when loading from Hugging Face\n",
    "KAZSANDRA_TEXT_COLUMN = 'text'\n",
    "KAZSANDRA_LABEL_COLUMN = 'label'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "46e6b9f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "AUTO_INSTALL_FASTTEXT = True  # toggle to attempt `pip install fasttext-wheel` (set False to skip installs)\n",
    "AUTO_DOWNLOAD_VECTORS = True  # toggle to download cc.kk.300.bin (~1.2GB); set False if you already have the file or are offline\n",
    "FASTTEXT_INSTALL_PACKAGE = 'fasttext-wheel'\n",
    "FASTTEXT_DOWNLOAD_URL = 'https://dl.fbaipublicfiles.com/fasttext/vectors-crawl/cc.kk.300.bin.gz'\n",
    "\n",
    "def ensure_fasttext_installed(auto_install: bool = False) -> bool:\n",
    "    global FASTTEXT_AVAILABLE\n",
    "    if FASTTEXT_AVAILABLE:\n",
    "        return True\n",
    "    if not auto_install:\n",
    "        print(\n",
    "            'fastText Python bindings are not installed. Install via `pip install fasttext-wheel` '\n",
    "            'or set AUTO_INSTALL_FASTTEXT=True to let the notebook attempt installation.'\n",
    "        )\n",
    "        return False\n",
    "    try:\n",
    "        import subprocess\n",
    "        import sys\n",
    "\n",
    "        subprocess.check_call([sys.executable, '-m', 'pip', 'install', FASTTEXT_INSTALL_PACKAGE])\n",
    "        FASTTEXT_AVAILABLE = importlib.util.find_spec('fasttext') is not None\n",
    "    except Exception as exc:  # noqa: BLE001\n",
    "        print(f'Automatic installation failed: {exc}')\n",
    "        FASTTEXT_AVAILABLE = False\n",
    "    return FASTTEXT_AVAILABLE\n",
    "\n",
    "def download_fasttext_vectors(target_path: Path, url: str) -> bool:\n",
    "    import gzip\n",
    "    import shutil\n",
    "    import urllib.request\n",
    "\n",
    "    target_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    gz_path = target_path.with_suffix(target_path.suffix + '.gz')\n",
    "    try:\n",
    "        with urllib.request.urlopen(url) as resp, open(gz_path, 'wb') as download_out:\n",
    "            shutil.copyfileobj(resp, download_out)\n",
    "        with gzip.open(gz_path, 'rb') as src, open(target_path, 'wb') as dst:\n",
    "            shutil.copyfileobj(src, dst)\n",
    "        return True\n",
    "    except Exception as exc:  # noqa: BLE001\n",
    "        print(f'Downloading fastText vectors failed: {exc}')\n",
    "        return False\n",
    "    finally:\n",
    "        if gz_path.exists():\n",
    "            try:\n",
    "                gz_path.unlink()\n",
    "            except OSError:\n",
    "                pass\n",
    "\n",
    "def ensure_vector_file(target_path: Path, url: str, auto_download: bool = False) -> bool:\n",
    "    if target_path.exists():\n",
    "        return True\n",
    "    if not auto_download:\n",
    "        print(\n",
    "            f'Vector file not found at {target_path}. '\n",
    "            'Set AUTO_DOWNLOAD_VECTORS=True to download automatically or place it manually.'\n",
    "        )\n",
    "        return False\n",
    "    print(f'Downloading fastText vectors from {url} (this is ~1.2GB)...')\n",
    "    return download_fasttext_vectors(target_path, url)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce1609f0",
   "metadata": {},
   "source": [
    "## Data loading helpers\n",
    "\n",
    "The helpers below mirror the logic used in the baseline scripts (`scripts/evaluate_language_id_baselines.py`) but trim it down for quick experimentation inside the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5d74c846",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                text              label\n",
      "0                           Itokasi Àwọn ástẹ́rọ́ìdì   yoruba heuristic\n",
      "1  Focusing on oneself is not listening, reading,...  english heuristic\n",
      "2                                                ...  english heuristic\n",
      "3  Төрт мезгіл тамақтану анағұрлым тиімді болып т...      kazakh stanza\n",
      "4  Жамбыл гидромелиоративтік-құрылыс институтын (...      kazakh stanza\n",
      "Class distribution: yoruba heuristic     2000\n",
      "english heuristic    2000\n",
      "kazakh stanza        2000\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "@dataclass\n",
    "class SentenceExample:\n",
    "    text: str\n",
    "    label: str\n",
    "\n",
    "\n",
    "def iter_conllu_sentences(path: Path) -> Iterable[str]:\n",
    "    buffer: List[str] = []\n",
    "    for line in path.read_text(encoding='utf8').splitlines():\n",
    "        if line.startswith('# text = '):\n",
    "            buffer.append(line[len('# text = ') :])\n",
    "        elif line.startswith('#'):\n",
    "            continue\n",
    "        elif not line.strip():\n",
    "            if buffer:\n",
    "                yield ' '.join(buffer).strip()\n",
    "                buffer = []\n",
    "        else:\n",
    "            continue\n",
    "    if buffer:\n",
    "        yield ' '.join(buffer).strip()\n",
    "\n",
    "\n",
    "def ensure_datasets_installed(auto_install: bool = False) -> bool:\n",
    "    global DATASETS_AVAILABLE\n",
    "    if DATASETS_AVAILABLE:\n",
    "        return True\n",
    "    if not auto_install:\n",
    "        print(\n",
    "            'Hugging Face datasets is not installed. Install manually via `pip install datasets` '\n",
    "            'or set AUTO_INSTALL_DATASETS=True.'\n",
    "        )\n",
    "        return False\n",
    "    try:\n",
    "        import subprocess\n",
    "        import sys\n",
    "\n",
    "        subprocess.check_call([sys.executable, '-m', 'pip', 'install', 'datasets'])\n",
    "        DATASETS_AVAILABLE = importlib.util.find_spec('datasets') is not None\n",
    "    except Exception as exc:  # noqa: BLE001\n",
    "        print(f'Automatic installation of `datasets` failed: {exc}')\n",
    "        DATASETS_AVAILABLE = False\n",
    "    return DATASETS_AVAILABLE\n",
    "\n",
    "\n",
    "def load_multilingual_dataset(\n",
    "    data_root: Path,\n",
    "    languages: Optional[Sequence[str]] = None,\n",
    "    max_sentences_per_language: Optional[int] = None,\n",
    "    seed: int = 13,\n",
    ") -> pd.DataFrame:\n",
    "    rng = random.Random(seed)\n",
    "    examples: List[SentenceExample] = []\n",
    "    language_dirs = sorted([p for p in data_root.iterdir() if p.is_dir()])\n",
    "    for lang_dir in language_dirs:\n",
    "        language = lang_dir.name\n",
    "        if languages and language not in languages:\n",
    "            continue\n",
    "        sentences: List[str] = []\n",
    "        for conllu_file in sorted(lang_dir.glob('*.conllu')):\n",
    "            sentences.extend(iter_conllu_sentences(conllu_file))\n",
    "        if max_sentences_per_language is not None:\n",
    "            rng.shuffle(sentences)\n",
    "            sentences = sentences[:max_sentences_per_language]\n",
    "        examples.extend(SentenceExample(text=s, label=language) for s in sentences)\n",
    "    rng.shuffle(examples)\n",
    "    return pd.DataFrame([vars(example) for example in examples], columns=['text', 'label'])\n",
    "\n",
    "\n",
    "def load_kazsandra_dataset(\n",
    "    local_path: Path,\n",
    "    hf_dataset: str,\n",
    "    dataset_config: str = 'full',\n",
    "    split: str = 'train',\n",
    "    auto_download: bool = False,\n",
    "    text_column: str = 'text',\n",
    "    label_column: str = 'label',\n",
    ") -> pd.DataFrame:\n",
    "    if local_path.exists():\n",
    "        df = pd.read_csv(local_path)\n",
    "        source = local_path\n",
    "    else:\n",
    "        if not ensure_datasets_installed(auto_download):\n",
    "            return pd.DataFrame(columns=['text', 'label'])\n",
    "        try:\n",
    "            from datasets import load_dataset\n",
    "\n",
    "            hf_ds = load_dataset(hf_dataset, dataset_config, split=split)\n",
    "            df = hf_ds.to_pandas()\n",
    "            source = f'{hf_dataset}/{dataset_config}::{split}'\n",
    "        except Exception as exc:  # noqa: BLE001\n",
    "            print(\n",
    "                'Kazakh sentiment dataset could not be loaded. '\n",
    "                'If you have a local CSV, set KAZSANDRA_LOCAL_PATH accordingly. '\n",
    "                f'Underlying error: {exc}'\n",
    "            )\n",
    "            return pd.DataFrame(columns=['text', 'label'])\n",
    "    if text_column not in df or label_column not in df:\n",
    "        print(\n",
    "            f'Expected columns `{text_column}` and `{label_column}` were not found in {source}. '\n",
    "            'Update KAZSANDRA_TEXT_COLUMN/KAZSANDRA_LABEL_COLUMN to match your export.'\n",
    "        )\n",
    "        return pd.DataFrame(columns=['text', 'label'])\n",
    "    df = df[[text_column, label_column]].rename(columns={text_column: 'text', label_column: 'label'})\n",
    "    df = df.dropna(subset=['text', 'label'])\n",
    "    print(\n",
    "        f'Loaded {len(df):,} Kazakh sentiment records from {source} using config `{dataset_config}` and split `{split}`.'\n",
    "    )\n",
    "    return df\n",
    "\n",
    "\n",
    "def preview_class_balance(df: pd.DataFrame) -> pd.Series:\n",
    "    counts = Counter(df['label'])\n",
    "    return pd.Series(counts).sort_values(ascending=False)\n",
    "\n",
    "\n",
    "wikipedia_dataset = load_multilingual_dataset(DATA_ROOT, languages=LANGUAGES, max_sentences_per_language=2000)\n",
    "if wikipedia_dataset.empty:\n",
    "    raise RuntimeError(\n",
    "        'No sentences were loaded from DATA_ROOT. Ensure the corpus is available and LANGUAGES is set correctly.'\n",
    "    )\n",
    "print('Wikipedia-derived sample:')\n",
    "print(wikipedia_dataset.head())\n",
    "print('Class distribution:', preview_class_balance(wikipedia_dataset))\n",
    "\n",
    "kazakh_sentiment_df = pd.DataFrame()\n",
    "if USE_KAZSANDRA:\n",
    "    kazakh_sentiment_df = load_kazsandra_dataset(\n",
    "        KAZSANDRA_LOCAL_PATH,\n",
    "        KAZSANDRA_HF_DATASET,\n",
    "        dataset_config=KAZSANDRA_HF_CONFIG,\n",
    "        split=KAZSANDRA_HF_SPLIT,\n",
    "        auto_download=AUTO_DOWNLOAD_KAZSANDRA,\n",
    "        text_column=KAZSANDRA_TEXT_COLUMN,\n",
    "        label_column=KAZSANDRA_LABEL_COLUMN,\n",
    "    )\n",
    "\n",
    "if not kazakh_sentiment_df.empty:\n",
    "    dataset = kazakh_sentiment_df\n",
    "    print('Using KazSAnDRA sentiment corpus for probing fastText embeddings.')\n",
    "else:\n",
    "    dataset = wikipedia_dataset\n",
    "    print('Falling back to Wikipedia-derived multilingual dataset.')\n",
    "print('Active dataset label balance:')\n",
    "print(preview_class_balance(dataset))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cell above now supports swapping in the KazSAnDRA sentiment corpus. If the dataset is available, it overrides the default Wikipedia snippets so downstream evaluation reflects sentiment-focused data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "882bebea",
   "metadata": {},
   "source": [
    "## Load fastText vectors\n",
    "\n",
    "The cell below loads a local binary `.bin` file with subword vectors. If the file is missing or the `fasttext` package is unavailable, the notebook surfaces clear guidance on how to proceed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e9492cf1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded fastText model with 300 dimensions from C:\\Users\\Maxim\\vectors\\cc.kk.300.bin\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "fasttext_model = None\n",
    "fasttext_dim = None\n",
    "\n",
    "fasttext_ready = ensure_fasttext_installed(AUTO_INSTALL_FASTTEXT)\n",
    "vector_ready = ensure_vector_file(FASTTEXT_VECTOR_PATH, FASTTEXT_DOWNLOAD_URL, AUTO_DOWNLOAD_VECTORS)\n",
    "\n",
    "if fasttext_ready and vector_ready:\n",
    "    import fasttext  # type: ignore\n",
    "\n",
    "    fasttext_model = fasttext.load_model(str(FASTTEXT_VECTOR_PATH))\n",
    "    fasttext_dim = fasttext_model.get_dimension()\n",
    "    print(f'Loaded fastText model with {fasttext_dim} dimensions from {FASTTEXT_VECTOR_PATH}')\n",
    "else:\n",
    "    guidance = []\n",
    "    if not fasttext_ready:\n",
    "        guidance.append(\n",
    "            '- fastText Python bindings are missing. Set AUTO_INSTALL_FASTTEXT=True or install manually via `pip install fasttext-wheel`.'\n",
    "        )\n",
    "    if not vector_ready:\n",
    "        guidance.append(\n",
    "            f'- fastText vector binary not found at {FASTTEXT_VECTOR_PATH}. Set AUTO_DOWNLOAD_VECTORS=True to fetch it or place it manually.'\n",
    "        )\n",
    "    raise RuntimeError(\n",
    "        'fastText setup is incomplete; please address the items below before rerunning:\\n' + '\\n'.join(guidance)\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ecf3c16",
   "metadata": {},
   "source": [
    "## Feature construction and model training\n",
    "\n",
    "Sentences are tokenised on whitespace and averaged into a single embedding vector. This mirrors the lightweight fastText probing described in the report (averaging static vectors before a linear classifier)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "772b2548",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Maxim\\conda\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {\n",
       "  /* Definition of color scheme common for light and dark mode */\n",
       "  --sklearn-color-text: #000;\n",
       "  --sklearn-color-text-muted: #666;\n",
       "  --sklearn-color-line: gray;\n",
       "  /* Definition of color scheme for unfitted estimators */\n",
       "  --sklearn-color-unfitted-level-0: #fff5e6;\n",
       "  --sklearn-color-unfitted-level-1: #f6e4d2;\n",
       "  --sklearn-color-unfitted-level-2: #ffe0b3;\n",
       "  --sklearn-color-unfitted-level-3: chocolate;\n",
       "  /* Definition of color scheme for fitted estimators */\n",
       "  --sklearn-color-fitted-level-0: #f0f8ff;\n",
       "  --sklearn-color-fitted-level-1: #d4ebff;\n",
       "  --sklearn-color-fitted-level-2: #b3dbfd;\n",
       "  --sklearn-color-fitted-level-3: cornflowerblue;\n",
       "\n",
       "  /* Specific color for light theme */\n",
       "  --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, white)));\n",
       "  --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-icon: #696969;\n",
       "\n",
       "  @media (prefers-color-scheme: dark) {\n",
       "    /* Redefinition of color scheme for dark theme */\n",
       "    --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, #111)));\n",
       "    --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-icon: #878787;\n",
       "  }\n",
       "}\n",
       "\n",
       "#sk-container-id-1 {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 pre {\n",
       "  padding: 0;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 input.sk-hidden--visually {\n",
       "  border: 0;\n",
       "  clip: rect(1px 1px 1px 1px);\n",
       "  clip: rect(1px, 1px, 1px, 1px);\n",
       "  height: 1px;\n",
       "  margin: -1px;\n",
       "  overflow: hidden;\n",
       "  padding: 0;\n",
       "  position: absolute;\n",
       "  width: 1px;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-dashed-wrapped {\n",
       "  border: 1px dashed var(--sklearn-color-line);\n",
       "  margin: 0 0.4em 0.5em 0.4em;\n",
       "  box-sizing: border-box;\n",
       "  padding-bottom: 0.4em;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-container {\n",
       "  /* jupyter's `normalize.less` sets `[hidden] { display: none; }`\n",
       "     but bootstrap.min.css set `[hidden] { display: none !important; }`\n",
       "     so we also need the `!important` here to be able to override the\n",
       "     default hidden behavior on the sphinx rendered scikit-learn.org.\n",
       "     See: https://github.com/scikit-learn/scikit-learn/issues/21755 */\n",
       "  display: inline-block !important;\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-text-repr-fallback {\n",
       "  display: none;\n",
       "}\n",
       "\n",
       "div.sk-parallel-item,\n",
       "div.sk-serial,\n",
       "div.sk-item {\n",
       "  /* draw centered vertical line to link estimators */\n",
       "  background-image: linear-gradient(var(--sklearn-color-text-on-default-background), var(--sklearn-color-text-on-default-background));\n",
       "  background-size: 2px 100%;\n",
       "  background-repeat: no-repeat;\n",
       "  background-position: center center;\n",
       "}\n",
       "\n",
       "/* Parallel-specific style estimator block */\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item::after {\n",
       "  content: \"\";\n",
       "  width: 100%;\n",
       "  border-bottom: 2px solid var(--sklearn-color-text-on-default-background);\n",
       "  flex-grow: 1;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel {\n",
       "  display: flex;\n",
       "  align-items: stretch;\n",
       "  justify-content: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item:first-child::after {\n",
       "  align-self: flex-end;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item:last-child::after {\n",
       "  align-self: flex-start;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item:only-child::after {\n",
       "  width: 0;\n",
       "}\n",
       "\n",
       "/* Serial-specific style estimator block */\n",
       "\n",
       "#sk-container-id-1 div.sk-serial {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "  align-items: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  padding-right: 1em;\n",
       "  padding-left: 1em;\n",
       "}\n",
       "\n",
       "\n",
       "/* Toggleable style: style used for estimator/Pipeline/ColumnTransformer box that is\n",
       "clickable and can be expanded/collapsed.\n",
       "- Pipeline and ColumnTransformer use this feature and define the default style\n",
       "- Estimators will overwrite some part of the style using the `sk-estimator` class\n",
       "*/\n",
       "\n",
       "/* Pipeline and ColumnTransformer style (default) */\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable {\n",
       "  /* Default theme specific background. It is overwritten whether we have a\n",
       "  specific estimator or a Pipeline/ColumnTransformer */\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "/* Toggleable label */\n",
       "#sk-container-id-1 label.sk-toggleable__label {\n",
       "  cursor: pointer;\n",
       "  display: flex;\n",
       "  width: 100%;\n",
       "  margin-bottom: 0;\n",
       "  padding: 0.5em;\n",
       "  box-sizing: border-box;\n",
       "  text-align: center;\n",
       "  align-items: start;\n",
       "  justify-content: space-between;\n",
       "  gap: 0.5em;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 label.sk-toggleable__label .caption {\n",
       "  font-size: 0.6rem;\n",
       "  font-weight: lighter;\n",
       "  color: var(--sklearn-color-text-muted);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 label.sk-toggleable__label-arrow:before {\n",
       "  /* Arrow on the left of the label */\n",
       "  content: \"▸\";\n",
       "  float: left;\n",
       "  margin-right: 0.25em;\n",
       "  color: var(--sklearn-color-icon);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "/* Toggleable content - dropdown */\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content {\n",
       "  max-height: 0;\n",
       "  max-width: 0;\n",
       "  overflow: hidden;\n",
       "  text-align: left;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content pre {\n",
       "  margin: 0.2em;\n",
       "  border-radius: 0.25em;\n",
       "  color: var(--sklearn-color-text);\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content.fitted pre {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {\n",
       "  /* Expand drop-down */\n",
       "  max-height: 200px;\n",
       "  max-width: 100%;\n",
       "  overflow: auto;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {\n",
       "  content: \"▾\";\n",
       "}\n",
       "\n",
       "/* Pipeline/ColumnTransformer-specific style */\n",
       "\n",
       "#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-label.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator-specific style */\n",
       "\n",
       "/* Colorize estimator box */\n",
       "#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-estimator.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-label label.sk-toggleable__label,\n",
       "#sk-container-id-1 div.sk-label label {\n",
       "  /* The background is the default theme color */\n",
       "  color: var(--sklearn-color-text-on-default-background);\n",
       "}\n",
       "\n",
       "/* On hover, darken the color of the background */\n",
       "#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "/* Label box, darken color on hover, fitted */\n",
       "#sk-container-id-1 div.sk-label.fitted:hover label.sk-toggleable__label.fitted {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator label */\n",
       "\n",
       "#sk-container-id-1 div.sk-label label {\n",
       "  font-family: monospace;\n",
       "  font-weight: bold;\n",
       "  display: inline-block;\n",
       "  line-height: 1.2em;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-label-container {\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       "/* Estimator-specific */\n",
       "#sk-container-id-1 div.sk-estimator {\n",
       "  font-family: monospace;\n",
       "  border: 1px dotted var(--sklearn-color-border-box);\n",
       "  border-radius: 0.25em;\n",
       "  box-sizing: border-box;\n",
       "  margin-bottom: 0.5em;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-estimator.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "/* on hover */\n",
       "#sk-container-id-1 div.sk-estimator:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-estimator.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Specification for estimator info (e.g. \"i\" and \"?\") */\n",
       "\n",
       "/* Common style for \"i\" and \"?\" */\n",
       "\n",
       ".sk-estimator-doc-link,\n",
       "a:link.sk-estimator-doc-link,\n",
       "a:visited.sk-estimator-doc-link {\n",
       "  float: right;\n",
       "  font-size: smaller;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1em;\n",
       "  height: 1em;\n",
       "  width: 1em;\n",
       "  text-decoration: none !important;\n",
       "  margin-left: 0.5em;\n",
       "  text-align: center;\n",
       "  /* unfitted */\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted,\n",
       "a:link.sk-estimator-doc-link.fitted,\n",
       "a:visited.sk-estimator-doc-link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "div.sk-estimator:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "div.sk-estimator.fitted:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "/* Span, style for the box shown on hovering the info icon */\n",
       ".sk-estimator-doc-link span {\n",
       "  display: none;\n",
       "  z-index: 9999;\n",
       "  position: relative;\n",
       "  font-weight: normal;\n",
       "  right: .2ex;\n",
       "  padding: .5ex;\n",
       "  margin: .5ex;\n",
       "  width: min-content;\n",
       "  min-width: 20ex;\n",
       "  max-width: 50ex;\n",
       "  color: var(--sklearn-color-text);\n",
       "  box-shadow: 2pt 2pt 4pt #999;\n",
       "  /* unfitted */\n",
       "  background: var(--sklearn-color-unfitted-level-0);\n",
       "  border: .5pt solid var(--sklearn-color-unfitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted span {\n",
       "  /* fitted */\n",
       "  background: var(--sklearn-color-fitted-level-0);\n",
       "  border: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link:hover span {\n",
       "  display: block;\n",
       "}\n",
       "\n",
       "/* \"?\"-specific style due to the `<a>` HTML tag */\n",
       "\n",
       "#sk-container-id-1 a.estimator_doc_link {\n",
       "  float: right;\n",
       "  font-size: 1rem;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1rem;\n",
       "  height: 1rem;\n",
       "  width: 1rem;\n",
       "  text-decoration: none;\n",
       "  /* unfitted */\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 a.estimator_doc_link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "#sk-container-id-1 a.estimator_doc_link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 a.estimator_doc_link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LogisticRegression(max_iter=1000, multi_class=&#x27;auto&#x27;, n_jobs=-1)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow\"><div><div>LogisticRegression</div></div><div><a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.6/modules/generated/sklearn.linear_model.LogisticRegression.html\">?<span>Documentation for LogisticRegression</span></a><span class=\"sk-estimator-doc-link fitted\">i<span>Fitted</span></span></div></label><div class=\"sk-toggleable__content fitted\"><pre>LogisticRegression(max_iter=1000, multi_class=&#x27;auto&#x27;, n_jobs=-1)</pre></div> </div></div></div></div>"
      ],
      "text/plain": [
       "LogisticRegression(max_iter=1000, multi_class='auto', n_jobs=-1)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def sentence_to_vector(text: str, model, dim: int) -> np.ndarray:\n",
    "    tokens = text.strip().split()\n",
    "    if not tokens:\n",
    "        return np.zeros(dim, dtype=np.float32)\n",
    "    vectors = [model.get_word_vector(tok) for tok in tokens]\n",
    "    return np.mean(np.stack(vectors, axis=0), axis=0)\n",
    "\n",
    "\n",
    "def build_embedding_matrix(texts: Sequence[str], model, dim: int) -> np.ndarray:\n",
    "    return np.vstack([sentence_to_vector(text, model, dim) for text in texts])\n",
    "\n",
    "\n",
    "if fasttext_model is None:\n",
    "    raise RuntimeError(\n",
    "        'A fastText model is required to continue. Resolve the setup issues above (installation or vector download) and rerun the loader cell.'\n",
    "    )\n",
    "\n",
    "X = build_embedding_matrix(dataset['text'], fasttext_model, fasttext_dim)\n",
    "y = dataset['label'].to_numpy()\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "classifier = LogisticRegression(max_iter=1000, n_jobs=-1, multi_class='auto')\n",
    "classifier.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "832725dd",
   "metadata": {},
   "source": [
    "## Evaluation and error inspection\n",
    "\n",
    "Accuracy and macro-averaged precision/recall/F1 provide a quick snapshot of how well fastText embeddings separate the selected languages. Misclassifications are shown to facilitate the qualitative inspection suggested in the report."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "adad9a43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy: 0.9483\n",
      "\n",
      "                   precision    recall  f1-score   support\n",
      "\n",
      "english heuristic       0.91      0.95      0.93       400\n",
      "    kazakh stanza       0.98      0.99      0.98       400\n",
      " yoruba heuristic       0.96      0.91      0.93       400\n",
      "\n",
      "         accuracy                           0.95      1200\n",
      "        macro avg       0.95      0.95      0.95      1200\n",
      "     weighted avg       0.95      0.95      0.95      1200\n",
      "\n",
      "Sample misclassifications (head):\n",
      "                                                text               gold  \\\n",
      "0                                                ...  english heuristic   \n",
      "1                                    Аузы- қосжақты.      kazakh stanza   \n",
      "2                                      Heerlen, N.V.  english heuristic   \n",
      "3  Durkheim, Marx, and the German theorist Max We...   yoruba heuristic   \n",
      "4  Get to the fucking point.\" Brooks came into hi...   yoruba heuristic   \n",
      "\n",
      "                pred  \n",
      "0      kazakh stanza  \n",
      "1   yoruba heuristic  \n",
      "2   yoruba heuristic  \n",
      "3  english heuristic  \n",
      "4  english heuristic  \n"
     ]
    }
   ],
   "source": [
    "y_pred = classifier.predict(X_test)\n",
    "print(f'Test accuracy: {accuracy_score(y_test, y_pred):.4f}\\n')\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "errors = []\n",
    "for text, gold, pred in zip(dataset['text'], y, classifier.predict(X)):\n",
    "    if gold != pred:\n",
    "        errors.append({'text': text, 'gold': gold, 'pred': pred})\n",
    "\n",
    "error_df = pd.DataFrame(errors)\n",
    "print('Sample misclassifications (head):')\n",
    "print(error_df.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de220367",
   "metadata": {},
   "source": [
    "## Next steps\n",
    "\n",
    "* Swap in domain-specific corpora (e.g., Yoruba social media posts) by replacing the `DATA_ROOT` path or loading an external dataframe.\n",
    "* Point `FASTTEXT_VECTOR_PATH` at the matching pretrained vectors (such as `cc.kk.300.bin` for Kazakh or `cc.yo.300.bin` for Yoruba) to mirror the report's planned experiments.\n",
    "* Extend the analysis by saving confusion matrices or integrating alternative feature baselines (character n-grams) to quantify the gap between static embeddings and more robust representations."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}