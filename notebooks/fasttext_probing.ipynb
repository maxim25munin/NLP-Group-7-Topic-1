{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# fastText probing notebook\n",
    "\n",
    "This notebook operationalises the experiments sketched in `reports/fasttext_limitations_and_kazakh.md`. It provides a reproducible pipeline for:\n",
    "\n",
    "1. Loading multilingual Wikipedia snippets from the repository's `data/` directory.\n",
    "2. Wiring in pretrained fastText vectors (e.g., the Kazakh `cc.kk.300.bin` model) to create sentence-level embeddings.\n",
    "3. Training a simple logistic regression classifier on averaged fastText vectors.\n",
    "4. Reporting held-out performance and surfacing misclassified examples for error analysis.\n",
    "\n",
    "> **Environment note:** The report mentions that package/model downloads were blocked in the grading environment. The notebook therefore detects whether fastText vectors are present locally and explains how to add them if they are missing."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "from pathlib import Path\n",
    "import importlib.util\n",
    "import json\n",
    "from collections import Counter\n",
    "from dataclasses import dataclass\n",
    "from typing import Iterable, List, Optional, Sequence\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "FASTTEXT_AVAILABLE = importlib.util.find_spec('fasttext') is not None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configure data and vectors\n\n",
    "Set the languages to evaluate and the location of your pretrained fastText vectors. The defaults work with the repository's Wikipedia-derived dataset and expect a local copy of the Kazakh vectors. Replace the paths with other language models (e.g., Yoruba) as needed."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "DATA_ROOT = Path('data')\n",
    "LANGUAGES = ['kazakh', 'yoruba', 'english']  # adjust to probe different subsets\n",
    "FASTTEXT_VECTOR_PATH = Path('vectors/cc.kk.300.bin')  # update if you store vectors elsewhere\n",
    "\n",
    "print(f'fastText installed: {FASTTEXT_AVAILABLE}')\n",
    "print(f'Vector file present: {FASTTEXT_VECTOR_PATH.exists()} ({FASTTEXT_VECTOR_PATH})')\n",
    "if not FASTTEXT_AVAILABLE:\n",
    "    print('Set AUTO_INSTALL_FASTTEXT=True to let the notebook try installing the package via pip.')\n",
    "if not FASTTEXT_VECTOR_PATH.exists():\n",
    "    print('Set AUTO_DOWNLOAD_VECTORS=True to fetch the Kazakh fastText vectors automatically (large download).')\n",
    "if not FASTTEXT_AVAILABLE or not FASTTEXT_VECTOR_PATH.exists():\n",
    "    print('The cells below now default to enabling both toggles so setup can continue automatically unless you opt out.')\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "AUTO_INSTALL_FASTTEXT = True  # toggle to attempt `pip install fasttext-wheel` (set False to skip installs)\n",
    "AUTO_DOWNLOAD_VECTORS = True  # toggle to download cc.kk.300.bin (~1.2GB); set False if you already have the file or are offline\n",
    "FASTTEXT_INSTALL_PACKAGE = 'fasttext-wheel'\n",
    "FASTTEXT_DOWNLOAD_URL = 'https://dl.fbaipublicfiles.com/fasttext/vectors-crawl/cc.kk.300.bin.gz'\n",
    "\n",
    "def ensure_fasttext_installed(auto_install: bool = False) -> bool:\n",
    "    global FASTTEXT_AVAILABLE\n",
    "    if FASTTEXT_AVAILABLE:\n",
    "        return True\n",
    "    if not auto_install:\n",
    "        print(\n",
    "            'fastText Python bindings are not installed. Install via `pip install fasttext-wheel` '\n",
    "            'or set AUTO_INSTALL_FASTTEXT=True to let the notebook attempt installation.'\n",
    "        )\n",
    "        return False\n",
    "    try:\n",
    "        import subprocess\n",
    "        import sys\n",
    "\n",
    "        subprocess.check_call([sys.executable, '-m', 'pip', 'install', FASTTEXT_INSTALL_PACKAGE])\n",
    "        FASTTEXT_AVAILABLE = importlib.util.find_spec('fasttext') is not None\n",
    "    except Exception as exc:  # noqa: BLE001\n",
    "        print(f'Automatic installation failed: {exc}')\n",
    "        FASTTEXT_AVAILABLE = False\n",
    "    return FASTTEXT_AVAILABLE\n",
    "\n",
    "def download_fasttext_vectors(target_path: Path, url: str) -> bool:\n",
    "    import gzip\n",
    "    import shutil\n",
    "    import urllib.request\n",
    "\n",
    "    target_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    gz_path = target_path.with_suffix(target_path.suffix + '.gz')\n",
    "    try:\n",
    "        with urllib.request.urlopen(url) as resp, open(gz_path, 'wb') as download_out:\n",
    "            shutil.copyfileobj(resp, download_out)\n",
    "        with gzip.open(gz_path, 'rb') as src, open(target_path, 'wb') as dst:\n",
    "            shutil.copyfileobj(src, dst)\n",
    "        return True\n",
    "    except Exception as exc:  # noqa: BLE001\n",
    "        print(f'Downloading fastText vectors failed: {exc}')\n",
    "        return False\n",
    "    finally:\n",
    "        if gz_path.exists():\n",
    "            try:\n",
    "                gz_path.unlink()\n",
    "            except OSError:\n",
    "                pass\n",
    "\n",
    "def ensure_vector_file(target_path: Path, url: str, auto_download: bool = False) -> bool:\n",
    "    if target_path.exists():\n",
    "        return True\n",
    "    if not auto_download:\n",
    "        print(\n",
    "            f'Vector file not found at {target_path}. '\n",
    "            'Set AUTO_DOWNLOAD_VECTORS=True to download automatically or place it manually.'\n",
    "        )\n",
    "        return False\n",
    "    print(f'Downloading fastText vectors from {url} (this is ~1.2GB)...')\n",
    "    return download_fasttext_vectors(target_path, url)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data loading helpers\n\n",
    "The helpers below mirror the logic used in the baseline scripts (`scripts/evaluate_language_id_baselines.py`) but trim it down for quick experimentation inside the notebook."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class SentenceExample:\n",
    "    text: str\n",
    "    label: str\n",
    "\n",
    "\n",
    "def iter_conllu_sentences(path: Path) -> Iterable[str]:\n",
    "    buffer: List[str] = []\n",
    "    for line in path.read_text(encoding='utf8').splitlines():\n",
    "        if line.startswith('# text = '):\n",
    "            buffer.append(line[len('# text = ') :])\n",
    "        elif line.startswith('#'):\n",
    "            continue\n",
    "        elif not line.strip():\n",
    "            if buffer:\n",
    "                yield ' '.join(buffer).strip()\n",
    "                buffer = []\n",
    "    if buffer:\n",
    "        yield ' '.join(buffer).strip()\n",
    "\n",
    "\n",
    "def load_multilingual_dataset(\n",
    "    data_root: Path,\n",
    "    languages: Optional[Sequence[str]] = None,\n",
    "    max_sentences_per_language: Optional[int] = None,\n",
    ") -> pd.DataFrame:\n",
    "    examples: List[SentenceExample] = []\n",
    "    language_dirs = sorted([p for p in data_root.iterdir() if p.is_dir()])\n",
    "    for lang_dir in language_dirs:\n",
    "        if languages and lang_dir.name not in languages:\n",
    "            continue\n",
    "        sentences: List[str] = []\n",
    "        for conllu_file in sorted(lang_dir.glob('*.conllu')):\n",
    "            sentences.extend(iter_conllu_sentences(conllu_file))\n",
    "        if max_sentences_per_language is not None:\n",
    "            sentences = sentences[:max_sentences_per_language]\n",
    "        examples.extend(SentenceExample(text=s, label=lang_dir.name) for s in sentences)\n",
    "    rng = np.random.default_rng(13)\n",
    "    rng.shuffle(examples)\n",
    "    return pd.DataFrame([vars(example) for example in examples], columns=[\"text\", \"label\"])\n",
    "\n",
    "\n",
    "def preview_class_balance(df: pd.DataFrame) -> pd.Series:\n",
    "    counts = Counter(df['label'])\n",
    "    return pd.Series(counts).sort_values(ascending=False)\n",
    "\n",
    "\n",
    "dataset = load_multilingual_dataset(DATA_ROOT, languages=LANGUAGES, max_sentences_per_language=2000)\n",
    "print(dataset.head())\n",
    "print('Class distribution:', preview_class_balance(dataset))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load fastText vectors\n\n",
    "The cell below loads a local binary `.bin` file with subword vectors. If the file is missing or the `fasttext` package is unavailable, the notebook surfaces clear guidance on how to proceed."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "fasttext_model = None\n",
    "fasttext_dim = None\n",
    "\n",
    "fasttext_ready = ensure_fasttext_installed(AUTO_INSTALL_FASTTEXT)\n",
    "vector_ready = ensure_vector_file(FASTTEXT_VECTOR_PATH, FASTTEXT_DOWNLOAD_URL, AUTO_DOWNLOAD_VECTORS)\n",
    "\n",
    "if fasttext_ready and vector_ready:\n",
    "    import fasttext  # type: ignore\n",
    "\n",
    "    fasttext_model = fasttext.load_model(str(FASTTEXT_VECTOR_PATH))\n",
    "    fasttext_dim = fasttext_model.get_dimension()\n",
    "    print(f'Loaded fastText model with {fasttext_dim} dimensions from {FASTTEXT_VECTOR_PATH}')\n",
    "else:\n",
    "    guidance = []\n",
    "    if not fasttext_ready:\n",
    "        guidance.append(\n",
    "            '- fastText Python bindings are missing. Set AUTO_INSTALL_FASTTEXT=True or install manually via `pip install fasttext-wheel`.'\n",
    "        )\n",
    "    if not vector_ready:\n",
    "        guidance.append(\n",
    "            f'- fastText vector binary not found at {FASTTEXT_VECTOR_PATH}. Set AUTO_DOWNLOAD_VECTORS=True to fetch it or place it manually.'\n",
    "        )\n",
    "    raise RuntimeError(\n",
    "        'fastText setup is incomplete; please address the items below before rerunning:\\n' + '\\n'.join(guidance)\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature construction and model training\n\n",
    "Sentences are tokenised on whitespace and averaged into a single embedding vector. This mirrors the lightweight fastText probing described in the report (averaging static vectors before a linear classifier)."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "def sentence_to_vector(text: str, model, dim: int) -> np.ndarray:\n",
    "    tokens = text.strip().split()\n",
    "    if not tokens:\n",
    "        return np.zeros(dim, dtype=np.float32)\n",
    "    vectors = [model.get_word_vector(tok) for tok in tokens]\n",
    "    return np.mean(np.stack(vectors, axis=0), axis=0)\n",
    "\n",
    "\n",
    "def build_embedding_matrix(texts: Sequence[str], model, dim: int) -> np.ndarray:\n",
    "    return np.vstack([sentence_to_vector(text, model, dim) for text in texts])\n",
    "\n",
    "\n",
    "if fasttext_model is None:\n",
    "    raise RuntimeError(\n",
    "        'A fastText model is required to continue. Resolve the setup issues above (installation or vector download) and rerun the loader cell.'\n",
    "    )\n",
    "\n",
    "X = build_embedding_matrix(dataset['text'], fasttext_model, fasttext_dim)\n",
    "y = dataset['label'].to_numpy()\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "classifier = LogisticRegression(max_iter=1000, n_jobs=-1, multi_class='auto')\n",
    "classifier.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation and error inspection\n\n",
    "Accuracy and macro-averaged precision/recall/F1 provide a quick snapshot of how well fastText embeddings separate the selected languages. Misclassifications are shown to facilitate the qualitative inspection suggested in the report."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "y_pred = classifier.predict(X_test)\n",
    "print(f'Test accuracy: {accuracy_score(y_test, y_pred):.4f}\\n')\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "errors = []\n",
    "for text, gold, pred in zip(dataset['text'], y, classifier.predict(X)):\n",
    "    if gold != pred:\n",
    "        errors.append({'text': text, 'gold': gold, 'pred': pred})\n",
    "\n",
    "error_df = pd.DataFrame(errors)\n",
    "print('Sample misclassifications (head):')\n",
    "print(error_df.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next steps\n\n",
    "* Swap in domain-specific corpora (e.g., Yoruba social media posts) by replacing the `DATA_ROOT` path or loading an external dataframe.\n",
    "* Point `FASTTEXT_VECTOR_PATH` at the matching pretrained vectors (such as `cc.kk.300.bin` for Kazakh or `cc.yo.300.bin` for Yoruba) to mirror the report's planned experiments.\n",
    "* Extend the analysis by saving confusion matrices or integrating alternative feature baselines (character n-grams) to quantify the gap between static embeddings and more robust representations."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}