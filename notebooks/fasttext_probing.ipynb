{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# fastText probing notebook\n",
        "\n",
        "This notebook operationalises the experiments sketched in `reports/fasttext_limitations_and_kazakh.md`. It provides a reproducible pipeline for:\n",
        "\n",
        "1. Loading multilingual Wikipedia snippets from the repository's `data/` directory.\n",
        "2. Wiring in pretrained fastText vectors (e.g., the Kazakh `cc.kk.300.bin` model) to create sentence-level embeddings.\n",
        "3. Training a simple logistic regression classifier on averaged fastText vectors.\n",
        "4. Reporting held-out performance and surfacing misclassified examples for error analysis.\n",
        "\n",
        "> **Environment note:** The report mentions that package/model downloads were blocked in the grading environment. The notebook therefore detects whether fastText vectors are present locally and explains how to add them if they are missing."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "from pathlib import Path\n",
        "import importlib.util\n",
        "import json\n",
        "from collections import Counter\n",
        "from dataclasses import dataclass\n",
        "from typing import Iterable, List, Optional, Sequence\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "FASTTEXT_AVAILABLE = importlib.util.find_spec('fasttext') is not None\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Configure data and vectors\n\n",
        "Set the languages to evaluate and the location of your pretrained fastText vectors. The defaults work with the repository's Wikipedia-derived dataset and expect a local copy of the Kazakh vectors. Replace the paths with other language models (e.g., Yoruba) as needed."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "DATA_ROOT = Path('data')\n",
        "LANGUAGES = ['kazakh', 'yoruba', 'english']  # adjust to probe different subsets\n",
        "FASTTEXT_VECTOR_PATH = Path('vectors/cc.kk.300.bin')  # update if you store vectors elsewhere\n",
        "\n",
        "print(f'fastText installed: {FASTTEXT_AVAILABLE}')\n",
        "print(f'Vector file present: {FASTTEXT_VECTOR_PATH.exists()} ({FASTTEXT_VECTOR_PATH})')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Data loading helpers\n\n",
        "The helpers below mirror the logic used in the baseline scripts (`scripts/evaluate_language_id_baselines.py`) but trim it down for quick experimentation inside the notebook."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "@dataclass\n",
        "class SentenceExample:\n",
        "    text: str\n",
        "    label: str\n",
        "\n",
        "\n",
        "def iter_conllu_sentences(path: Path) -> Iterable[str]:\n",
        "    buffer: List[str] = []\n",
        "    for line in path.read_text(encoding='utf8').splitlines():\n",
        "        if line.startswith('# text = '):\n",
        "            buffer.append(line[len('# text = ') :])\n",
        "        elif line.startswith('#'):\n",
        "            continue\n",
        "        elif not line.strip():\n",
        "            if buffer:\n",
        "                yield ' '.join(buffer).strip()\n",
        "                buffer = []\n",
        "    if buffer:\n",
        "        yield ' '.join(buffer).strip()\n",
        "\n",
        "\n",
        "def load_multilingual_dataset(\n",
        "    data_root: Path,\n",
        "    languages: Optional[Sequence[str]] = None,\n",
        "    max_sentences_per_language: Optional[int] = None,\n",
        ") -> pd.DataFrame:\n",
        "    examples: List[SentenceExample] = []\n",
        "    language_dirs = sorted([p for p in data_root.iterdir() if p.is_dir()])\n",
        "    for lang_dir in language_dirs:\n",
        "        if languages and lang_dir.name not in languages:\n",
        "            continue\n",
        "        sentences: List[str] = []\n",
        "        for conllu_file in sorted(lang_dir.glob('*.conllu')):\n",
        "            sentences.extend(iter_conllu_sentences(conllu_file))\n",
        "        if max_sentences_per_language is not None:\n",
        "            sentences = sentences[:max_sentences_per_language]\n",
        "        examples.extend(SentenceExample(text=s, label=lang_dir.name) for s in sentences)\n",
        "    rng = np.random.default_rng(13)\n",
        "    rng.shuffle(examples)\n",
        "    return pd.DataFrame(examples)\n",
        "\n",
        "\n",
        "def preview_class_balance(df: pd.DataFrame) -> pd.Series:\n",
        "    counts = Counter(df['label'])\n",
        "    return pd.Series(counts).sort_values(ascending=False)\n",
        "\n",
        "\n",
        "dataset = load_multilingual_dataset(DATA_ROOT, languages=LANGUAGES, max_sentences_per_language=2000)\n",
        "print(dataset.head())\n",
        "print('\nClass distribution:\n', preview_class_balance(dataset))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Load fastText vectors\n\n",
        "The cell below loads a local binary `.bin` file with subword vectors. If the file is missing or the `fasttext` package is unavailable, the notebook surfaces clear guidance on how to proceed."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "fasttext_model = None\n",
        "fasttext_dim = None\n",
        "\n",
        "if FASTTEXT_AVAILABLE and FASTTEXT_VECTOR_PATH.exists():\n",
        "    import fasttext  # type: ignore\n",
        "\n",
        "    fasttext_model = fasttext.load_model(str(FASTTEXT_VECTOR_PATH))\n",
        "    fasttext_dim = fasttext_model.get_dimension()\n",
        "    print(f'Loaded fastText model with {fasttext_dim} dimensions from {FASTTEXT_VECTOR_PATH}')\n",
        "elif not FASTTEXT_AVAILABLE:\n",
        "    print(\n",
        "        'fastText Python bindings are not installed. Install via `pip install fasttext` ' \n",
        "        'and download the desired `.bin` vectors (e.g., cc.kk.300.bin) before rerunning.'\n",
        "    )\n",
        "else:\n",
        "    print(\n",
        "        f'Vector file not found at {FASTTEXT_VECTOR_PATH}. Place the pretrained fastText binary ' \n",
        "        'there or update FASTTEXT_VECTOR_PATH to point at your local copy.'\n",
        "    )\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Feature construction and model training\n\n",
        "Sentences are tokenised on whitespace and averaged into a single embedding vector. This mirrors the lightweight fastText probing described in the report (averaging static vectors before a linear classifier)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "def sentence_to_vector(text: str, model, dim: int) -> np.ndarray:\n",
        "    tokens = text.strip().split()\n",
        "    if not tokens:\n",
        "        return np.zeros(dim, dtype=np.float32)\n",
        "    vectors = [model.get_word_vector(tok) for tok in tokens]\n",
        "    return np.mean(np.stack(vectors, axis=0), axis=0)\n",
        "\n",
        "\n",
        "def build_embedding_matrix(texts: Sequence[str], model, dim: int) -> np.ndarray:\n",
        "    return np.vstack([sentence_to_vector(text, model, dim) for text in texts])\n",
        "\n",
        "\n",
        "if fasttext_model is None:\n",
        "    raise RuntimeError(\n",
        "        'A fastText model is required to continue. Please ensure FASTTEXT_VECTOR_PATH points ' \n",
        "        'to a valid .bin file and that the `fasttext` package is installed.'\n",
        "    )\n",
        "\n",
        "X = build_embedding_matrix(dataset['text'], fasttext_model, fasttext_dim)\n",
        "y = dataset['label'].to_numpy()\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
        "\n",
        "classifier = LogisticRegression(max_iter=1000, n_jobs=-1, multi_class='auto')\n",
        "classifier.fit(X_train, y_train)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Evaluation and error inspection\n\n",
        "Accuracy and macro-averaged precision/recall/F1 provide a quick snapshot of how well fastText embeddings separate the selected languages. Misclassifications are shown to facilitate the qualitative inspection suggested in the report."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "y_pred = classifier.predict(X_test)\n",
        "print(f'Test accuracy: {accuracy_score(y_test, y_pred):.4f}\\n')\n",
        "print(classification_report(y_test, y_pred))\n",
        "\n",
        "errors = []\n",
        "for text, gold, pred in zip(dataset['text'], y, classifier.predict(X)):\n",
        "    if gold != pred:\n",
        "        errors.append({'text': text, 'gold': gold, 'pred': pred})\n",
        "\n",
        "error_df = pd.DataFrame(errors)\n",
        "print('Sample misclassifications (head):')\n",
        "print(error_df.head())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Next steps\n\n",
        "* Swap in domain-specific corpora (e.g., Yoruba social media posts) by replacing the `DATA_ROOT` path or loading an external dataframe.\n",
        "* Point `FASTTEXT_VECTOR_PATH` at the matching pretrained vectors (such as `cc.kk.300.bin` for Kazakh or `cc.yo.300.bin` for Yoruba) to mirror the report's planned experiments.\n",
        "* Extend the analysis by saving confusion matrices or integrating alternative feature baselines (character n-grams) to quantify the gap between static embeddings and more robust representations."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}